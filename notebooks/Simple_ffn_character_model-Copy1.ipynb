{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple feed forward model\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "- [Read data](#Read-data)\n",
    "- [Prepare data](#Prepare-data)\n",
    "- [Create and train model](#Create-and-train-model)\n",
    "- [Test on unseen data](#Test-on-unseen-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODULES_PATH = '../modules'\n",
    "MODELS_PATH = '../models'\n",
    "DATA_PATH = '../data'\n",
    "\n",
    "sys.path.append(MODULES_PATH)\n",
    "from data import flatten_data, prepare_training_data, prepare_test_data, \\\n",
    "                    raise_one_level\n",
    "from models import parameter_ffn_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH,'sentences.json'),'r') as datafile:\n",
    "    sentences = json.load(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = pd.read_csv('../data/training_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_flat = raise_one_level(sentences)\n",
    "sentences_df = pd.DataFrame(sentences_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpora_train, corpora_test, labels_train, labels_test = train_test_split(\n",
    "                                                                        sentences_df['body'],\n",
    "                                                                        sentences_df['class'],\n",
    "                                                                        test_size=0.25,\n",
    "                                                                        random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "for i in range(1,2):\n",
    "    print(i+1)\n",
    "\n",
    "    document_matrix, labels, pipeline_instance = prepare_training_data(corpora_train, labels_train, (i,i))\n",
    "    training_data.append({'document_matrix': document_matrix, 'labels': labels, 'pipeline_instance': pipeline_instance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10139, 68), (10139, 11), Pipeline(memory=None,\n",
       "      steps=[('vect', CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]['document_matrix'].shape, training_data[0]['labels'].shape, training_data[0]['pipeline_instance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10139, 68)\n",
      "(10139, 2161)\n",
      "(10139, 21082)\n"
     ]
    }
   ],
   "source": [
    "for i in training_data:\n",
    "    print(i['document_matrix'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, 'pipeline_instance.pickle'),'wb') as datafile:\n",
    "        pickle.dump(pipeline_instance, datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ffn = KerasClassifier(build_fn=parameter_ffn_seq, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'layers': [],\n",
    "                   'activations': [['relu']],\n",
    "                   'dropout': [[0.05], [0.15], [0.25]],\n",
    "                   'attention': [128],\n",
    "             'input_shape': [document_matrix.iloc[0:5].shape[1]], \n",
    "              'nb_classes': [labels.iloc[0:5].shape[1]]}\n",
    "for j in [64, 128, 256, 512, 1024, 2048]:\n",
    "    for i in range(3):\n",
    "        parameters['layers'].append([j]*(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activations': [['relu']],\n",
       " 'attention': [128],\n",
       " 'dropout': [[0.05], [0.15], [0.25]],\n",
       " 'input_shape': [68],\n",
       " 'layers': [[64],\n",
       "  [64, 64],\n",
       "  [64, 64, 64],\n",
       "  [128],\n",
       "  [128, 128],\n",
       "  [128, 128, 128],\n",
       "  [256],\n",
       "  [256, 256],\n",
       "  [256, 256, 256],\n",
       "  [512],\n",
       "  [512, 512],\n",
       "  [512, 512, 512],\n",
       "  [1024],\n",
       "  [1024, 1024],\n",
       "  [1024, 1024, 1024],\n",
       "  [2048],\n",
       "  [2048, 2048],\n",
       "  [2048, 2048, 2048]],\n",
       " 'nb_classes': [11]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ffn_grid = GridSearchCV(estimator=ffn, param_grid=parameters, n_jobs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Thembanip/Documents/py/sa-language-classifier/sa_lang_env/lib/python3.5/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=3)]: Done 162 out of 162 | elapsed: 30.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.8008 - acc: 0.4510\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.8498 - acc: 0.7260\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6512 - acc: 0.7890\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5713 - acc: 0.7990\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.5035 - acc: 0.8260\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.4512 - acc: 0.8490\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 0.4198 - acc: 0.8550\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.3894 - acc: 0.8650\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3661 - acc: 0.8790\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3539 - acc: 0.8760\n"
     ]
    }
   ],
   "source": [
    "grid_result = ffn_grid.fit(document_matrix, labels, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Thembanip/Documents/py/sa-language-classifier/sa_lang_env/lib/python3.5/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/mnt/c/Users/Thembanip/Documents/py/sa-language-classifier/sa_lang_env/lib/python3.5/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/mnt/c/Users/Thembanip/Documents/py/sa-language-classifier/sa_lang_env/lib/python3.5/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/mnt/c/Users/Thembanip/Documents/py/sa-language-classifier/sa_lang_env/lib/python3.5/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/mnt/c/Users/Thembanip/Documents/py/sa-language-classifier/sa_lang_env/lib/python3.5/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_activations</th>\n",
       "      <th>param_attention</th>\n",
       "      <th>param_dropout</th>\n",
       "      <th>param_input_shape</th>\n",
       "      <th>param_layers</th>\n",
       "      <th>param_nb_classes</th>\n",
       "      <th>...</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>42.768974</td>\n",
       "      <td>1.487677</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.946000</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>68</td>\n",
       "      <td>[1024, 1024]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814371</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.938531</td>\n",
       "      <td>0.792793</td>\n",
       "      <td>0.953523</td>\n",
       "      <td>0.059467</td>\n",
       "      <td>0.085348</td>\n",
       "      <td>0.012839</td>\n",
       "      <td>0.006121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>100.736751</td>\n",
       "      <td>4.622157</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>68</td>\n",
       "      <td>[2048, 2048]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820359</td>\n",
       "      <td>0.926426</td>\n",
       "      <td>0.795796</td>\n",
       "      <td>0.917541</td>\n",
       "      <td>0.768769</td>\n",
       "      <td>0.934033</td>\n",
       "      <td>0.294189</td>\n",
       "      <td>0.500056</td>\n",
       "      <td>0.021074</td>\n",
       "      <td>0.006739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>223.831091</td>\n",
       "      <td>2.026304</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.966505</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>68</td>\n",
       "      <td>[2048, 2048, 2048]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808383</td>\n",
       "      <td>0.977477</td>\n",
       "      <td>0.789790</td>\n",
       "      <td>0.961019</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.961019</td>\n",
       "      <td>0.348028</td>\n",
       "      <td>0.060322</td>\n",
       "      <td>0.012595</td>\n",
       "      <td>0.007758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>121.860950</td>\n",
       "      <td>1.745150</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.963001</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>68</td>\n",
       "      <td>[2048, 2048]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811377</td>\n",
       "      <td>0.965465</td>\n",
       "      <td>0.795796</td>\n",
       "      <td>0.967016</td>\n",
       "      <td>0.768769</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.333983</td>\n",
       "      <td>0.029222</td>\n",
       "      <td>0.017605</td>\n",
       "      <td>0.004625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>121.558473</td>\n",
       "      <td>3.723957</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.948996</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.15]</td>\n",
       "      <td>68</td>\n",
       "      <td>[2048, 2048]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799401</td>\n",
       "      <td>0.941441</td>\n",
       "      <td>0.792793</td>\n",
       "      <td>0.946027</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.959520</td>\n",
       "      <td>6.145709</td>\n",
       "      <td>0.740501</td>\n",
       "      <td>0.007708</td>\n",
       "      <td>0.007673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>214.099664</td>\n",
       "      <td>4.318652</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.951494</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.15]</td>\n",
       "      <td>68</td>\n",
       "      <td>[2048, 2048, 2048]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787425</td>\n",
       "      <td>0.939940</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.959520</td>\n",
       "      <td>0.768769</td>\n",
       "      <td>0.955022</td>\n",
       "      <td>2.167750</td>\n",
       "      <td>0.806629</td>\n",
       "      <td>0.017191</td>\n",
       "      <td>0.008374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>24.862370</td>\n",
       "      <td>3.771168</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.876497</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>68</td>\n",
       "      <td>[1024, 1024]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802395</td>\n",
       "      <td>0.870871</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.880060</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.878561</td>\n",
       "      <td>0.542790</td>\n",
       "      <td>0.303048</td>\n",
       "      <td>0.010194</td>\n",
       "      <td>0.004025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>174.396906</td>\n",
       "      <td>5.391669</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.909499</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>68</td>\n",
       "      <td>[2048, 2048, 2048]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790419</td>\n",
       "      <td>0.906907</td>\n",
       "      <td>0.774775</td>\n",
       "      <td>0.911544</td>\n",
       "      <td>0.786787</td>\n",
       "      <td>0.910045</td>\n",
       "      <td>0.195293</td>\n",
       "      <td>0.837150</td>\n",
       "      <td>0.006685</td>\n",
       "      <td>0.001932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>48.326230</td>\n",
       "      <td>2.711005</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.907499</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.15]</td>\n",
       "      <td>68</td>\n",
       "      <td>[1024, 1024, 1024]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793413</td>\n",
       "      <td>0.905405</td>\n",
       "      <td>0.795796</td>\n",
       "      <td>0.904048</td>\n",
       "      <td>0.762763</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.101066</td>\n",
       "      <td>0.192890</td>\n",
       "      <td>0.015037</td>\n",
       "      <td>0.003960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>68.088104</td>\n",
       "      <td>1.612082</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.958498</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>68</td>\n",
       "      <td>[1024, 1024, 1024]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778443</td>\n",
       "      <td>0.954955</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.953523</td>\n",
       "      <td>0.762763</td>\n",
       "      <td>0.967016</td>\n",
       "      <td>0.264536</td>\n",
       "      <td>0.111811</td>\n",
       "      <td>0.016032</td>\n",
       "      <td>0.006052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9.865702</td>\n",
       "      <td>1.242853</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.839992</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>68</td>\n",
       "      <td>[2048]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790419</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.774775</td>\n",
       "      <td>0.842579</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.853073</td>\n",
       "      <td>0.323395</td>\n",
       "      <td>0.099540</td>\n",
       "      <td>0.006782</td>\n",
       "      <td>0.011878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>27.732585</td>\n",
       "      <td>2.545931</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.909499</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.15]</td>\n",
       "      <td>68</td>\n",
       "      <td>[1024, 1024]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781437</td>\n",
       "      <td>0.908408</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.911544</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.908546</td>\n",
       "      <td>0.212696</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>0.001447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>39.828962</td>\n",
       "      <td>4.356670</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.870990</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>68</td>\n",
       "      <td>[1024, 1024, 1024]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781437</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.765766</td>\n",
       "      <td>0.865067</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.372482</td>\n",
       "      <td>0.686864</td>\n",
       "      <td>0.006694</td>\n",
       "      <td>0.018922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21.870544</td>\n",
       "      <td>0.895705</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.910501</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>68</td>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784431</td>\n",
       "      <td>0.912913</td>\n",
       "      <td>0.762763</td>\n",
       "      <td>0.901049</td>\n",
       "      <td>0.768769</td>\n",
       "      <td>0.917541</td>\n",
       "      <td>0.153751</td>\n",
       "      <td>0.049368</td>\n",
       "      <td>0.009138</td>\n",
       "      <td>0.006945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13.246642</td>\n",
       "      <td>0.862178</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.886498</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>68</td>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778443</td>\n",
       "      <td>0.882883</td>\n",
       "      <td>0.747748</td>\n",
       "      <td>0.892054</td>\n",
       "      <td>0.759760</td>\n",
       "      <td>0.884558</td>\n",
       "      <td>0.212467</td>\n",
       "      <td>0.111682</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>0.003988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>10.350144</td>\n",
       "      <td>2.179253</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.841994</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.15]</td>\n",
       "      <td>68</td>\n",
       "      <td>[2048]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.754491</td>\n",
       "      <td>0.830330</td>\n",
       "      <td>0.765766</td>\n",
       "      <td>0.845577</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.850075</td>\n",
       "      <td>0.135137</td>\n",
       "      <td>0.078035</td>\n",
       "      <td>0.004869</td>\n",
       "      <td>0.008450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>12.981018</td>\n",
       "      <td>3.243638</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.835503</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>68</td>\n",
       "      <td>[2048]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.775449</td>\n",
       "      <td>0.840841</td>\n",
       "      <td>0.759760</td>\n",
       "      <td>0.830585</td>\n",
       "      <td>0.732733</td>\n",
       "      <td>0.835082</td>\n",
       "      <td>0.316638</td>\n",
       "      <td>0.070167</td>\n",
       "      <td>0.017644</td>\n",
       "      <td>0.004198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17.173636</td>\n",
       "      <td>2.284263</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.835489</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.15]</td>\n",
       "      <td>68</td>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736527</td>\n",
       "      <td>0.813814</td>\n",
       "      <td>0.759760</td>\n",
       "      <td>0.851574</td>\n",
       "      <td>0.744745</td>\n",
       "      <td>0.841079</td>\n",
       "      <td>0.153778</td>\n",
       "      <td>0.288353</td>\n",
       "      <td>0.009620</td>\n",
       "      <td>0.015914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13.412730</td>\n",
       "      <td>2.235476</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.817496</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.15]</td>\n",
       "      <td>68</td>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736527</td>\n",
       "      <td>0.809309</td>\n",
       "      <td>0.750751</td>\n",
       "      <td>0.830585</td>\n",
       "      <td>0.720721</td>\n",
       "      <td>0.812594</td>\n",
       "      <td>0.426034</td>\n",
       "      <td>0.033027</td>\n",
       "      <td>0.012259</td>\n",
       "      <td>0.009352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>18.026502</td>\n",
       "      <td>3.648597</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.784006</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>68</td>\n",
       "      <td>[512, 512]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718563</td>\n",
       "      <td>0.795796</td>\n",
       "      <td>0.774775</td>\n",
       "      <td>0.802099</td>\n",
       "      <td>0.699700</td>\n",
       "      <td>0.754123</td>\n",
       "      <td>1.116626</td>\n",
       "      <td>0.416921</td>\n",
       "      <td>0.031875</td>\n",
       "      <td>0.021287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.756739</td>\n",
       "      <td>0.724576</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.784500</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>68</td>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709581</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.744745</td>\n",
       "      <td>0.790105</td>\n",
       "      <td>0.720721</td>\n",
       "      <td>0.779610</td>\n",
       "      <td>0.265865</td>\n",
       "      <td>0.009452</td>\n",
       "      <td>0.014674</td>\n",
       "      <td>0.004314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>18.982888</td>\n",
       "      <td>3.693631</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.773004</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>68</td>\n",
       "      <td>[512, 512, 512]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730539</td>\n",
       "      <td>0.780781</td>\n",
       "      <td>0.744745</td>\n",
       "      <td>0.773613</td>\n",
       "      <td>0.672673</td>\n",
       "      <td>0.764618</td>\n",
       "      <td>0.240996</td>\n",
       "      <td>0.296570</td>\n",
       "      <td>0.031159</td>\n",
       "      <td>0.006613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10.007906</td>\n",
       "      <td>2.131112</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.778996</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.15]</td>\n",
       "      <td>68</td>\n",
       "      <td>[1024]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727545</td>\n",
       "      <td>0.771772</td>\n",
       "      <td>0.714715</td>\n",
       "      <td>0.787106</td>\n",
       "      <td>0.705706</td>\n",
       "      <td>0.778111</td>\n",
       "      <td>0.058193</td>\n",
       "      <td>0.100879</td>\n",
       "      <td>0.008964</td>\n",
       "      <td>0.006292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>12.118799</td>\n",
       "      <td>3.074027</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.771495</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>68</td>\n",
       "      <td>[1024]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676647</td>\n",
       "      <td>0.761261</td>\n",
       "      <td>0.732733</td>\n",
       "      <td>0.755622</td>\n",
       "      <td>0.714715</td>\n",
       "      <td>0.797601</td>\n",
       "      <td>0.154321</td>\n",
       "      <td>0.052055</td>\n",
       "      <td>0.023389</td>\n",
       "      <td>0.018603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.182217</td>\n",
       "      <td>0.902658</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.775993</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>68</td>\n",
       "      <td>[1024]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.715569</td>\n",
       "      <td>0.762763</td>\n",
       "      <td>0.693694</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.693694</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.108282</td>\n",
       "      <td>0.015460</td>\n",
       "      <td>0.010317</td>\n",
       "      <td>0.009355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.366465</td>\n",
       "      <td>0.612909</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.743005</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>68</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709581</td>\n",
       "      <td>0.752252</td>\n",
       "      <td>0.690691</td>\n",
       "      <td>0.721139</td>\n",
       "      <td>0.657658</td>\n",
       "      <td>0.755622</td>\n",
       "      <td>0.164556</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>0.021460</td>\n",
       "      <td>0.015522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12.653657</td>\n",
       "      <td>1.955007</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.724496</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.15]</td>\n",
       "      <td>68</td>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670659</td>\n",
       "      <td>0.716216</td>\n",
       "      <td>0.660661</td>\n",
       "      <td>0.718141</td>\n",
       "      <td>0.708709</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.132314</td>\n",
       "      <td>0.004459</td>\n",
       "      <td>0.020692</td>\n",
       "      <td>0.010378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.654725</td>\n",
       "      <td>0.745505</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.702991</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>68</td>\n",
       "      <td>[512]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667665</td>\n",
       "      <td>0.684685</td>\n",
       "      <td>0.681682</td>\n",
       "      <td>0.706147</td>\n",
       "      <td>0.672673</td>\n",
       "      <td>0.718141</td>\n",
       "      <td>0.143269</td>\n",
       "      <td>0.091788</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.013840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9.566286</td>\n",
       "      <td>1.770681</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.701498</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.15]</td>\n",
       "      <td>68</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673653</td>\n",
       "      <td>0.698198</td>\n",
       "      <td>0.705706</td>\n",
       "      <td>0.710645</td>\n",
       "      <td>0.630631</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.130831</td>\n",
       "      <td>0.097026</td>\n",
       "      <td>0.030743</td>\n",
       "      <td>0.006550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8.312751</td>\n",
       "      <td>1.688733</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.688490</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.15]</td>\n",
       "      <td>68</td>\n",
       "      <td>[512]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634731</td>\n",
       "      <td>0.668168</td>\n",
       "      <td>0.672673</td>\n",
       "      <td>0.691154</td>\n",
       "      <td>0.663664</td>\n",
       "      <td>0.706147</td>\n",
       "      <td>0.108682</td>\n",
       "      <td>0.075574</td>\n",
       "      <td>0.016193</td>\n",
       "      <td>0.015619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>11.202146</td>\n",
       "      <td>3.140299</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.701999</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>68</td>\n",
       "      <td>[512]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673653</td>\n",
       "      <td>0.699700</td>\n",
       "      <td>0.669670</td>\n",
       "      <td>0.728636</td>\n",
       "      <td>0.603604</td>\n",
       "      <td>0.677661</td>\n",
       "      <td>0.132373</td>\n",
       "      <td>0.219648</td>\n",
       "      <td>0.032117</td>\n",
       "      <td>0.020874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>12.971073</td>\n",
       "      <td>3.093580</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.670503</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>68</td>\n",
       "      <td>[256, 256]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631737</td>\n",
       "      <td>0.677177</td>\n",
       "      <td>0.657658</td>\n",
       "      <td>0.662669</td>\n",
       "      <td>0.624625</td>\n",
       "      <td>0.671664</td>\n",
       "      <td>0.731122</td>\n",
       "      <td>0.188511</td>\n",
       "      <td>0.014190</td>\n",
       "      <td>0.005980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>15.384129</td>\n",
       "      <td>2.943063</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.619499</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>68</td>\n",
       "      <td>[256, 256, 256]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559880</td>\n",
       "      <td>0.617117</td>\n",
       "      <td>0.606607</td>\n",
       "      <td>0.587706</td>\n",
       "      <td>0.618619</td>\n",
       "      <td>0.653673</td>\n",
       "      <td>0.233312</td>\n",
       "      <td>0.064411</td>\n",
       "      <td>0.025349</td>\n",
       "      <td>0.026984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.099322</td>\n",
       "      <td>0.602979</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.584003</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>68</td>\n",
       "      <td>[256]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580838</td>\n",
       "      <td>0.590090</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.610195</td>\n",
       "      <td>0.519520</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.070568</td>\n",
       "      <td>0.219583</td>\n",
       "      <td>0.038620</td>\n",
       "      <td>0.024256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.986936</td>\n",
       "      <td>0.482995</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.597981</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>68</td>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526946</td>\n",
       "      <td>0.560060</td>\n",
       "      <td>0.552553</td>\n",
       "      <td>0.604198</td>\n",
       "      <td>0.600601</td>\n",
       "      <td>0.629685</td>\n",
       "      <td>0.037315</td>\n",
       "      <td>0.010636</td>\n",
       "      <td>0.030534</td>\n",
       "      <td>0.028762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.109233</td>\n",
       "      <td>0.341856</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.539007</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>68</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517964</td>\n",
       "      <td>0.552553</td>\n",
       "      <td>0.549550</td>\n",
       "      <td>0.539730</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>0.524738</td>\n",
       "      <td>0.017144</td>\n",
       "      <td>0.014963</td>\n",
       "      <td>0.037305</td>\n",
       "      <td>0.011367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8.909351</td>\n",
       "      <td>1.365283</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.511010</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.15]</td>\n",
       "      <td>68</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517964</td>\n",
       "      <td>0.530030</td>\n",
       "      <td>0.537538</td>\n",
       "      <td>0.500750</td>\n",
       "      <td>0.450450</td>\n",
       "      <td>0.502249</td>\n",
       "      <td>0.221840</td>\n",
       "      <td>0.063497</td>\n",
       "      <td>0.037290</td>\n",
       "      <td>0.013463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7.040590</td>\n",
       "      <td>1.774376</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.514466</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.15]</td>\n",
       "      <td>68</td>\n",
       "      <td>[256]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407186</td>\n",
       "      <td>0.445946</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.496252</td>\n",
       "      <td>0.552553</td>\n",
       "      <td>0.601199</td>\n",
       "      <td>0.055236</td>\n",
       "      <td>0.007555</td>\n",
       "      <td>0.061650</td>\n",
       "      <td>0.064677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>10.497318</td>\n",
       "      <td>2.757802</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.479998</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>68</td>\n",
       "      <td>[256]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.437126</td>\n",
       "      <td>0.475976</td>\n",
       "      <td>0.438438</td>\n",
       "      <td>0.430285</td>\n",
       "      <td>0.522523</td>\n",
       "      <td>0.533733</td>\n",
       "      <td>0.473903</td>\n",
       "      <td>0.026041</td>\n",
       "      <td>0.039941</td>\n",
       "      <td>0.042328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.778610</td>\n",
       "      <td>1.706620</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.506517</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.15]</td>\n",
       "      <td>68</td>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479042</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.489489</td>\n",
       "      <td>0.491754</td>\n",
       "      <td>0.429429</td>\n",
       "      <td>0.487256</td>\n",
       "      <td>0.172506</td>\n",
       "      <td>0.017971</td>\n",
       "      <td>0.026190</td>\n",
       "      <td>0.024128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>14.002287</td>\n",
       "      <td>2.919060</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.433497</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>68</td>\n",
       "      <td>[128, 128, 128]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380240</td>\n",
       "      <td>0.427928</td>\n",
       "      <td>0.438438</td>\n",
       "      <td>0.439280</td>\n",
       "      <td>0.429429</td>\n",
       "      <td>0.433283</td>\n",
       "      <td>0.043919</td>\n",
       "      <td>0.102078</td>\n",
       "      <td>0.025590</td>\n",
       "      <td>0.004637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.685426</td>\n",
       "      <td>0.224808</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.420031</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>68</td>\n",
       "      <td>[128]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407186</td>\n",
       "      <td>0.481982</td>\n",
       "      <td>0.426426</td>\n",
       "      <td>0.437781</td>\n",
       "      <td>0.327327</td>\n",
       "      <td>0.340330</td>\n",
       "      <td>0.058245</td>\n",
       "      <td>0.007158</td>\n",
       "      <td>0.042889</td>\n",
       "      <td>0.059176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>11.715556</td>\n",
       "      <td>2.721730</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.394958</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>68</td>\n",
       "      <td>[128, 128]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263473</td>\n",
       "      <td>0.310811</td>\n",
       "      <td>0.453453</td>\n",
       "      <td>0.451274</td>\n",
       "      <td>0.354354</td>\n",
       "      <td>0.422789</td>\n",
       "      <td>0.279203</td>\n",
       "      <td>0.008873</td>\n",
       "      <td>0.077601</td>\n",
       "      <td>0.060627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.472193</td>\n",
       "      <td>0.531989</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.374498</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>68</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299401</td>\n",
       "      <td>0.370871</td>\n",
       "      <td>0.357357</td>\n",
       "      <td>0.353823</td>\n",
       "      <td>0.384384</td>\n",
       "      <td>0.398801</td>\n",
       "      <td>0.155909</td>\n",
       "      <td>0.150282</td>\n",
       "      <td>0.035466</td>\n",
       "      <td>0.018540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.066013</td>\n",
       "      <td>0.155293</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.302528</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>68</td>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365269</td>\n",
       "      <td>0.358859</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.329835</td>\n",
       "      <td>0.252252</td>\n",
       "      <td>0.218891</td>\n",
       "      <td>0.024554</td>\n",
       "      <td>0.013650</td>\n",
       "      <td>0.050311</td>\n",
       "      <td>0.060316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>9.583459</td>\n",
       "      <td>2.641063</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.329005</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>68</td>\n",
       "      <td>[128]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320359</td>\n",
       "      <td>0.339339</td>\n",
       "      <td>0.210210</td>\n",
       "      <td>0.277361</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.370315</td>\n",
       "      <td>0.357326</td>\n",
       "      <td>0.077845</td>\n",
       "      <td>0.055219</td>\n",
       "      <td>0.038645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8.069599</td>\n",
       "      <td>1.217000</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.286470</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.15]</td>\n",
       "      <td>68</td>\n",
       "      <td>[64]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170659</td>\n",
       "      <td>0.226727</td>\n",
       "      <td>0.294294</td>\n",
       "      <td>0.305847</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.326837</td>\n",
       "      <td>0.733457</td>\n",
       "      <td>0.251335</td>\n",
       "      <td>0.075449</td>\n",
       "      <td>0.043105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7.124977</td>\n",
       "      <td>1.549667</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.272984</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.15]</td>\n",
       "      <td>68</td>\n",
       "      <td>[128]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209581</td>\n",
       "      <td>0.240240</td>\n",
       "      <td>0.411411</td>\n",
       "      <td>0.403298</td>\n",
       "      <td>0.171171</td>\n",
       "      <td>0.175412</td>\n",
       "      <td>0.157167</td>\n",
       "      <td>0.053526</td>\n",
       "      <td>0.105332</td>\n",
       "      <td>0.095872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>9.439291</td>\n",
       "      <td>2.351446</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.263021</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>68</td>\n",
       "      <td>[64]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.317365</td>\n",
       "      <td>0.304805</td>\n",
       "      <td>0.129129</td>\n",
       "      <td>0.154423</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.329835</td>\n",
       "      <td>0.481225</td>\n",
       "      <td>0.052314</td>\n",
       "      <td>0.084390</td>\n",
       "      <td>0.077467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8.429719</td>\n",
       "      <td>1.280823</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.268488</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.15]</td>\n",
       "      <td>68</td>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209581</td>\n",
       "      <td>0.244745</td>\n",
       "      <td>0.255255</td>\n",
       "      <td>0.236882</td>\n",
       "      <td>0.279279</td>\n",
       "      <td>0.323838</td>\n",
       "      <td>0.406106</td>\n",
       "      <td>0.103086</td>\n",
       "      <td>0.028919</td>\n",
       "      <td>0.039270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.495595</td>\n",
       "      <td>1.593713</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.263954</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.15]</td>\n",
       "      <td>68</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152695</td>\n",
       "      <td>0.171171</td>\n",
       "      <td>0.279279</td>\n",
       "      <td>0.296852</td>\n",
       "      <td>0.288288</td>\n",
       "      <td>0.323838</td>\n",
       "      <td>0.232475</td>\n",
       "      <td>0.038977</td>\n",
       "      <td>0.061936</td>\n",
       "      <td>0.066526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>13.367426</td>\n",
       "      <td>2.603077</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.230521</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>68</td>\n",
       "      <td>[64, 64, 64]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236527</td>\n",
       "      <td>0.271772</td>\n",
       "      <td>0.225225</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.168168</td>\n",
       "      <td>0.178411</td>\n",
       "      <td>0.167451</td>\n",
       "      <td>0.232915</td>\n",
       "      <td>0.029916</td>\n",
       "      <td>0.038880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.931486</td>\n",
       "      <td>0.115772</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.191012</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.05]</td>\n",
       "      <td>68</td>\n",
       "      <td>[64]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233533</td>\n",
       "      <td>0.214715</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.152924</td>\n",
       "      <td>0.228228</td>\n",
       "      <td>0.205397</td>\n",
       "      <td>0.145668</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.032460</td>\n",
       "      <td>0.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>11.052373</td>\n",
       "      <td>2.646160</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.150958</td>\n",
       "      <td>[relu]</td>\n",
       "      <td>128</td>\n",
       "      <td>[0.25]</td>\n",
       "      <td>68</td>\n",
       "      <td>[64, 64]</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083832</td>\n",
       "      <td>0.067568</td>\n",
       "      <td>0.243243</td>\n",
       "      <td>0.263868</td>\n",
       "      <td>0.123123</td>\n",
       "      <td>0.121439</td>\n",
       "      <td>0.366449</td>\n",
       "      <td>0.276831</td>\n",
       "      <td>0.067809</td>\n",
       "      <td>0.082813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "13      42.768974         1.487677            0.797          0.946000   \n",
       "52     100.736751         4.622157            0.795          0.926000   \n",
       "17     223.831091         2.026304            0.792          0.966505   \n",
       "16     121.860950         1.745150            0.792          0.963001   \n",
       "34     121.558473         3.723957            0.791          0.948996   \n",
       "35     214.099664         4.318652            0.789          0.951494   \n",
       "49      24.862370         3.771168            0.788          0.876497   \n",
       "53     174.396906         5.391669            0.784          0.909499   \n",
       "32      48.326230         2.711005            0.784          0.907499   \n",
       "14      68.088104         1.612082            0.781          0.958498   \n",
       "15       9.865702         1.242853            0.781          0.839992   \n",
       "31      27.732585         2.545931            0.780          0.909499   \n",
       "50      39.828962         4.356670            0.775          0.870990   \n",
       "11      21.870544         0.895705            0.772          0.910501   \n",
       "10      13.246642         0.862178            0.762          0.886498   \n",
       "33      10.350144         2.179253            0.759          0.841994   \n",
       "51      12.981018         3.243638            0.756          0.835503   \n",
       "29      17.173636         2.284263            0.747          0.835489   \n",
       "28      13.412730         2.235476            0.736          0.817496   \n",
       "46      18.026502         3.648597            0.731          0.784006   \n",
       "8       10.756739         0.724576            0.725          0.784500   \n",
       "47      18.982888         3.693631            0.716          0.773004   \n",
       "30      10.007906         2.131112            0.716          0.778996   \n",
       "48      12.118799         3.074027            0.708          0.771495   \n",
       "12       7.182217         0.902658            0.701          0.775993   \n",
       "7        7.366465         0.612909            0.686          0.743005   \n",
       "26      12.653657         1.955007            0.680          0.724496   \n",
       "9        5.654725         0.745505            0.674          0.702991   \n",
       "25       9.566286         1.770681            0.670          0.701498   \n",
       "27       8.312751         1.688733            0.657          0.688490   \n",
       "45      11.202146         3.140299            0.649          0.701999   \n",
       "43      12.971073         3.093580            0.638          0.670503   \n",
       "44      15.384129         2.943063            0.595          0.619499   \n",
       "6        5.099322         0.602979            0.571          0.584003   \n",
       "5        8.986936         0.482995            0.560          0.597981   \n",
       "4        6.109233         0.341856            0.509          0.539007   \n",
       "22       8.909351         1.365283            0.502          0.511010   \n",
       "24       7.040590         1.774376            0.468          0.514466   \n",
       "42      10.497318         2.757802            0.466          0.479998   \n",
       "23      10.778610         1.706620            0.466          0.506517   \n",
       "41      14.002287         2.919060            0.416          0.433497   \n",
       "3        3.685426         0.224808            0.387          0.420031   \n",
       "40      11.715556         2.721730            0.357          0.394958   \n",
       "2        6.472193         0.531989            0.347          0.374498   \n",
       "1        4.066013         0.155293            0.323          0.302528   \n",
       "39       9.583459         2.641063            0.288          0.329005   \n",
       "18       8.069599         1.217000            0.272          0.286470   \n",
       "21       7.124977         1.549667            0.264          0.272984   \n",
       "36       9.439291         2.351446            0.248          0.263021   \n",
       "19       8.429719         1.280823            0.248          0.268488   \n",
       "20      10.495595         1.593713            0.240          0.263954   \n",
       "38      13.367426         2.603077            0.210          0.230521   \n",
       "0        3.931486         0.115772            0.208          0.191012   \n",
       "37      11.052373         2.646160            0.150          0.150958   \n",
       "\n",
       "   param_activations param_attention param_dropout param_input_shape  \\\n",
       "13            [relu]             128        [0.05]                68   \n",
       "52            [relu]             128        [0.25]                68   \n",
       "17            [relu]             128        [0.05]                68   \n",
       "16            [relu]             128        [0.05]                68   \n",
       "34            [relu]             128        [0.15]                68   \n",
       "35            [relu]             128        [0.15]                68   \n",
       "49            [relu]             128        [0.25]                68   \n",
       "53            [relu]             128        [0.25]                68   \n",
       "32            [relu]             128        [0.15]                68   \n",
       "14            [relu]             128        [0.05]                68   \n",
       "15            [relu]             128        [0.05]                68   \n",
       "31            [relu]             128        [0.15]                68   \n",
       "50            [relu]             128        [0.25]                68   \n",
       "11            [relu]             128        [0.05]                68   \n",
       "10            [relu]             128        [0.05]                68   \n",
       "33            [relu]             128        [0.15]                68   \n",
       "51            [relu]             128        [0.25]                68   \n",
       "29            [relu]             128        [0.15]                68   \n",
       "28            [relu]             128        [0.15]                68   \n",
       "46            [relu]             128        [0.25]                68   \n",
       "8             [relu]             128        [0.05]                68   \n",
       "47            [relu]             128        [0.25]                68   \n",
       "30            [relu]             128        [0.15]                68   \n",
       "48            [relu]             128        [0.25]                68   \n",
       "12            [relu]             128        [0.05]                68   \n",
       "7             [relu]             128        [0.05]                68   \n",
       "26            [relu]             128        [0.15]                68   \n",
       "9             [relu]             128        [0.05]                68   \n",
       "25            [relu]             128        [0.15]                68   \n",
       "27            [relu]             128        [0.15]                68   \n",
       "45            [relu]             128        [0.25]                68   \n",
       "43            [relu]             128        [0.25]                68   \n",
       "44            [relu]             128        [0.25]                68   \n",
       "6             [relu]             128        [0.05]                68   \n",
       "5             [relu]             128        [0.05]                68   \n",
       "4             [relu]             128        [0.05]                68   \n",
       "22            [relu]             128        [0.15]                68   \n",
       "24            [relu]             128        [0.15]                68   \n",
       "42            [relu]             128        [0.25]                68   \n",
       "23            [relu]             128        [0.15]                68   \n",
       "41            [relu]             128        [0.25]                68   \n",
       "3             [relu]             128        [0.05]                68   \n",
       "40            [relu]             128        [0.25]                68   \n",
       "2             [relu]             128        [0.05]                68   \n",
       "1             [relu]             128        [0.05]                68   \n",
       "39            [relu]             128        [0.25]                68   \n",
       "18            [relu]             128        [0.15]                68   \n",
       "21            [relu]             128        [0.15]                68   \n",
       "36            [relu]             128        [0.25]                68   \n",
       "19            [relu]             128        [0.15]                68   \n",
       "20            [relu]             128        [0.15]                68   \n",
       "38            [relu]             128        [0.25]                68   \n",
       "0             [relu]             128        [0.05]                68   \n",
       "37            [relu]             128        [0.25]                68   \n",
       "\n",
       "          param_layers param_nb_classes       ...        split0_test_score  \\\n",
       "13        [1024, 1024]               11       ...                 0.814371   \n",
       "52        [2048, 2048]               11       ...                 0.820359   \n",
       "17  [2048, 2048, 2048]               11       ...                 0.808383   \n",
       "16        [2048, 2048]               11       ...                 0.811377   \n",
       "34        [2048, 2048]               11       ...                 0.799401   \n",
       "35  [2048, 2048, 2048]               11       ...                 0.787425   \n",
       "49        [1024, 1024]               11       ...                 0.802395   \n",
       "53  [2048, 2048, 2048]               11       ...                 0.790419   \n",
       "32  [1024, 1024, 1024]               11       ...                 0.793413   \n",
       "14  [1024, 1024, 1024]               11       ...                 0.778443   \n",
       "15              [2048]               11       ...                 0.790419   \n",
       "31        [1024, 1024]               11       ...                 0.781437   \n",
       "50  [1024, 1024, 1024]               11       ...                 0.781437   \n",
       "11     [512, 512, 512]               11       ...                 0.784431   \n",
       "10          [512, 512]               11       ...                 0.778443   \n",
       "33              [2048]               11       ...                 0.754491   \n",
       "51              [2048]               11       ...                 0.775449   \n",
       "29     [512, 512, 512]               11       ...                 0.736527   \n",
       "28          [512, 512]               11       ...                 0.736527   \n",
       "46          [512, 512]               11       ...                 0.718563   \n",
       "8      [256, 256, 256]               11       ...                 0.709581   \n",
       "47     [512, 512, 512]               11       ...                 0.730539   \n",
       "30              [1024]               11       ...                 0.727545   \n",
       "48              [1024]               11       ...                 0.676647   \n",
       "12              [1024]               11       ...                 0.715569   \n",
       "7           [256, 256]               11       ...                 0.709581   \n",
       "26     [256, 256, 256]               11       ...                 0.670659   \n",
       "9                [512]               11       ...                 0.667665   \n",
       "25          [256, 256]               11       ...                 0.673653   \n",
       "27               [512]               11       ...                 0.634731   \n",
       "45               [512]               11       ...                 0.673653   \n",
       "43          [256, 256]               11       ...                 0.631737   \n",
       "44     [256, 256, 256]               11       ...                 0.559880   \n",
       "6                [256]               11       ...                 0.580838   \n",
       "5      [128, 128, 128]               11       ...                 0.526946   \n",
       "4           [128, 128]               11       ...                 0.517964   \n",
       "22          [128, 128]               11       ...                 0.517964   \n",
       "24               [256]               11       ...                 0.407186   \n",
       "42               [256]               11       ...                 0.437126   \n",
       "23     [128, 128, 128]               11       ...                 0.479042   \n",
       "41     [128, 128, 128]               11       ...                 0.380240   \n",
       "3                [128]               11       ...                 0.407186   \n",
       "40          [128, 128]               11       ...                 0.263473   \n",
       "2         [64, 64, 64]               11       ...                 0.299401   \n",
       "1             [64, 64]               11       ...                 0.365269   \n",
       "39               [128]               11       ...                 0.320359   \n",
       "18                [64]               11       ...                 0.170659   \n",
       "21               [128]               11       ...                 0.209581   \n",
       "36                [64]               11       ...                 0.317365   \n",
       "19            [64, 64]               11       ...                 0.209581   \n",
       "20        [64, 64, 64]               11       ...                 0.152695   \n",
       "38        [64, 64, 64]               11       ...                 0.236527   \n",
       "0                 [64]               11       ...                 0.233533   \n",
       "37            [64, 64]               11       ...                 0.083832   \n",
       "\n",
       "    split0_train_score  split1_test_score  split1_train_score  \\\n",
       "13            0.945946           0.783784            0.938531   \n",
       "52            0.926426           0.795796            0.917541   \n",
       "17            0.977477           0.789790            0.961019   \n",
       "16            0.965465           0.795796            0.967016   \n",
       "34            0.941441           0.792793            0.946027   \n",
       "35            0.939940           0.810811            0.959520   \n",
       "49            0.870871           0.780781            0.880060   \n",
       "53            0.906907           0.774775            0.911544   \n",
       "32            0.905405           0.795796            0.904048   \n",
       "14            0.954955           0.801802            0.953523   \n",
       "15            0.824324           0.774775            0.842579   \n",
       "31            0.908408           0.777778            0.911544   \n",
       "50            0.851351           0.765766            0.865067   \n",
       "11            0.912913           0.762763            0.901049   \n",
       "10            0.882883           0.747748            0.892054   \n",
       "33            0.830330           0.765766            0.845577   \n",
       "51            0.840841           0.759760            0.830585   \n",
       "29            0.813814           0.759760            0.851574   \n",
       "28            0.809309           0.750751            0.830585   \n",
       "46            0.795796           0.774775            0.802099   \n",
       "8             0.783784           0.744745            0.790105   \n",
       "47            0.780781           0.744745            0.773613   \n",
       "30            0.771772           0.714715            0.787106   \n",
       "48            0.761261           0.732733            0.755622   \n",
       "12            0.762763           0.693694            0.782609   \n",
       "7             0.752252           0.690691            0.721139   \n",
       "26            0.716216           0.660661            0.718141   \n",
       "9             0.684685           0.681682            0.706147   \n",
       "25            0.698198           0.705706            0.710645   \n",
       "27            0.668168           0.672673            0.691154   \n",
       "45            0.699700           0.669670            0.728636   \n",
       "43            0.677177           0.657658            0.662669   \n",
       "44            0.617117           0.606607            0.587706   \n",
       "6             0.590090           0.612613            0.610195   \n",
       "5             0.560060           0.552553            0.604198   \n",
       "4             0.552553           0.549550            0.539730   \n",
       "22            0.530030           0.537538            0.500750   \n",
       "24            0.445946           0.444444            0.496252   \n",
       "42            0.475976           0.438438            0.430285   \n",
       "23            0.540541           0.489489            0.491754   \n",
       "41            0.427928           0.438438            0.439280   \n",
       "3             0.481982           0.426426            0.437781   \n",
       "40            0.310811           0.453453            0.451274   \n",
       "2             0.370871           0.357357            0.353823   \n",
       "1             0.358859           0.351351            0.329835   \n",
       "39            0.339339           0.210210            0.277361   \n",
       "18            0.226727           0.294294            0.305847   \n",
       "21            0.240240           0.411411            0.403298   \n",
       "36            0.304805           0.129129            0.154423   \n",
       "19            0.244745           0.255255            0.236882   \n",
       "20            0.171171           0.279279            0.296852   \n",
       "38            0.271772           0.225225            0.241379   \n",
       "0             0.214715           0.162162            0.152924   \n",
       "37            0.067568           0.243243            0.263868   \n",
       "\n",
       "    split2_test_score  split2_train_score  std_fit_time  std_score_time  \\\n",
       "13           0.792793            0.953523      0.059467        0.085348   \n",
       "52           0.768769            0.934033      0.294189        0.500056   \n",
       "17           0.777778            0.961019      0.348028        0.060322   \n",
       "16           0.768769            0.956522      0.333983        0.029222   \n",
       "34           0.780781            0.959520      6.145709        0.740501   \n",
       "35           0.768769            0.955022      2.167750        0.806629   \n",
       "49           0.780781            0.878561      0.542790        0.303048   \n",
       "53           0.786787            0.910045      0.195293        0.837150   \n",
       "32           0.762763            0.913043      0.101066        0.192890   \n",
       "14           0.762763            0.967016      0.264536        0.111811   \n",
       "15           0.777778            0.853073      0.323395        0.099540   \n",
       "31           0.780781            0.908546      0.212696        0.155200   \n",
       "50           0.777778            0.896552      0.372482        0.686864   \n",
       "11           0.768769            0.917541      0.153751        0.049368   \n",
       "10           0.759760            0.884558      0.212467        0.111682   \n",
       "33           0.756757            0.850075      0.135137        0.078035   \n",
       "51           0.732733            0.835082      0.316638        0.070167   \n",
       "29           0.744745            0.841079      0.153778        0.288353   \n",
       "28           0.720721            0.812594      0.426034        0.033027   \n",
       "46           0.699700            0.754123      1.116626        0.416921   \n",
       "8            0.720721            0.779610      0.265865        0.009452   \n",
       "47           0.672673            0.764618      0.240996        0.296570   \n",
       "30           0.705706            0.778111      0.058193        0.100879   \n",
       "48           0.714715            0.797601      0.154321        0.052055   \n",
       "12           0.693694            0.782609      0.108282        0.015460   \n",
       "7            0.657658            0.755622      0.164556        0.012925   \n",
       "26           0.708709            0.739130      0.132314        0.004459   \n",
       "9            0.672673            0.718141      0.143269        0.091788   \n",
       "25           0.630631            0.695652      0.130831        0.097026   \n",
       "27           0.663664            0.706147      0.108682        0.075574   \n",
       "45           0.603604            0.677661      0.132373        0.219648   \n",
       "43           0.624625            0.671664      0.731122        0.188511   \n",
       "44           0.618619            0.653673      0.233312        0.064411   \n",
       "6            0.519520            0.551724      0.070568        0.219583   \n",
       "5            0.600601            0.629685      0.037315        0.010636   \n",
       "4            0.459459            0.524738      0.017144        0.014963   \n",
       "22           0.450450            0.502249      0.221840        0.063497   \n",
       "24           0.552553            0.601199      0.055236        0.007555   \n",
       "42           0.522523            0.533733      0.473903        0.026041   \n",
       "23           0.429429            0.487256      0.172506        0.017971   \n",
       "41           0.429429            0.433283      0.043919        0.102078   \n",
       "3            0.327327            0.340330      0.058245        0.007158   \n",
       "40           0.354354            0.422789      0.279203        0.008873   \n",
       "2            0.384384            0.398801      0.155909        0.150282   \n",
       "1            0.252252            0.218891      0.024554        0.013650   \n",
       "39           0.333333            0.370315      0.357326        0.077845   \n",
       "18           0.351351            0.326837      0.733457        0.251335   \n",
       "21           0.171171            0.175412      0.157167        0.053526   \n",
       "36           0.297297            0.329835      0.481225        0.052314   \n",
       "19           0.279279            0.323838      0.406106        0.103086   \n",
       "20           0.288288            0.323838      0.232475        0.038977   \n",
       "38           0.168168            0.178411      0.167451        0.232915   \n",
       "0            0.228228            0.205397      0.145668        0.000854   \n",
       "37           0.123123            0.121439      0.366449        0.276831   \n",
       "\n",
       "    std_test_score  std_train_score  \n",
       "13        0.012839         0.006121  \n",
       "52        0.021074         0.006739  \n",
       "17        0.012595         0.007758  \n",
       "16        0.017605         0.004625  \n",
       "34        0.007708         0.007673  \n",
       "35        0.017191         0.008374  \n",
       "49        0.010194         0.004025  \n",
       "53        0.006685         0.001932  \n",
       "32        0.015037         0.003960  \n",
       "14        0.016032         0.006052  \n",
       "15        0.006782         0.011878  \n",
       "31        0.001593         0.001447  \n",
       "50        0.006694         0.018922  \n",
       "11        0.009138         0.006945  \n",
       "10        0.012634         0.003988  \n",
       "33        0.004869         0.008450  \n",
       "51        0.017644         0.004198  \n",
       "29        0.009620         0.015914  \n",
       "28        0.012259         0.009352  \n",
       "46        0.031875         0.021287  \n",
       "8         0.014674         0.004314  \n",
       "47        0.031159         0.006613  \n",
       "30        0.008964         0.006292  \n",
       "48        0.023389         0.018603  \n",
       "12        0.010317         0.009355  \n",
       "7         0.021460         0.015522  \n",
       "26        0.020692         0.010378  \n",
       "9         0.005800         0.013840  \n",
       "25        0.030743         0.006550  \n",
       "27        0.016193         0.015619  \n",
       "45        0.032117         0.020874  \n",
       "43        0.014190         0.005980  \n",
       "44        0.025349         0.026984  \n",
       "6         0.038620         0.024256  \n",
       "5         0.030534         0.028762  \n",
       "4         0.037305         0.011367  \n",
       "22        0.037290         0.013463  \n",
       "24        0.061650         0.064677  \n",
       "42        0.039941         0.042328  \n",
       "23        0.026190         0.024128  \n",
       "41        0.025590         0.004637  \n",
       "3         0.042889         0.059176  \n",
       "40        0.077601         0.060627  \n",
       "2         0.035466         0.018540  \n",
       "1         0.050311         0.060316  \n",
       "39        0.055219         0.038645  \n",
       "18        0.075449         0.043105  \n",
       "21        0.105332         0.095872  \n",
       "36        0.084390         0.077467  \n",
       "19        0.028919         0.039270  \n",
       "20        0.061936         0.066526  \n",
       "38        0.029916         0.038880  \n",
       "0         0.032460         0.027200  \n",
       "37        0.067809         0.082813  \n",
       "\n",
       "[54 rows x 22 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_result.cv_results_).sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 68)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 128)          8832        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128)          0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 128)          0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128)          512         dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 128)          16512       batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 128)          0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 128)          0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128)          512         dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 128)          16512       batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 128)          0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 128)          0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 128)          512         dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_probs (Dense)         (None, 128)          16512       batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Multiply)        (None, 128)          0           batch_normalization_14[0][0]     \n",
      "                                                                 attention_probs[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 11)           1419        attention_mul[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 11)           0           dense_20[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 61,323\n",
      "Trainable params: 60,555\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = parameter_ffn(document_matrix, labels, **{'layers': [128, 128, 128],\n",
    "                                                  'activations': ['relu'],\n",
    "                                                  'dropout': [0.15],\n",
    "                                                  'attention': 128})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10139, 68)\n",
      "Train on 9125 samples, validate on 1014 samples\n",
      "Epoch 1/100\n",
      "9125/9125 [==============================] - 1s 131us/step - loss: 2.3898 - acc: 0.2655 - val_loss: 2.3824 - val_acc: 0.3402\n",
      "Epoch 2/100\n",
      "9125/9125 [==============================] - 1s 89us/step - loss: 2.3738 - acc: 0.3468 - val_loss: 2.3614 - val_acc: 0.3205\n",
      "Epoch 3/100\n",
      "9125/9125 [==============================] - 1s 89us/step - loss: 2.3360 - acc: 0.3395 - val_loss: 2.3005 - val_acc: 0.3323\n",
      "Epoch 4/100\n",
      "9125/9125 [==============================] - 1s 68us/step - loss: 2.2720 - acc: 0.3530 - val_loss: 2.2307 - val_acc: 0.3728\n",
      "Epoch 5/100\n",
      "9125/9125 [==============================] - 1s 69us/step - loss: 2.2051 - acc: 0.3784 - val_loss: 2.1467 - val_acc: 0.3895\n",
      "Epoch 6/100\n",
      "9125/9125 [==============================] - 1s 69us/step - loss: 2.1287 - acc: 0.4169 - val_loss: 2.0746 - val_acc: 0.4783\n",
      "Epoch 7/100\n",
      "9125/9125 [==============================] - 1s 134us/step - loss: 2.0666 - acc: 0.4671 - val_loss: 2.0086 - val_acc: 0.5276\n",
      "Epoch 8/100\n",
      "9125/9125 [==============================] - 1s 69us/step - loss: 2.0053 - acc: 0.5022 - val_loss: 1.9493 - val_acc: 0.5690\n",
      "Epoch 9/100\n",
      "9125/9125 [==============================] - 1s 68us/step - loss: 1.9452 - acc: 0.5332 - val_loss: 1.8799 - val_acc: 0.6065\n",
      "Epoch 10/100\n",
      "9125/9125 [==============================] - 1s 69us/step - loss: 1.8805 - acc: 0.5638 - val_loss: 1.8100 - val_acc: 0.6243\n",
      "Epoch 11/100\n",
      "9125/9125 [==============================] - 1s 105us/step - loss: 1.8226 - acc: 0.5813 - val_loss: 1.7485 - val_acc: 0.6391\n",
      "Epoch 12/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 1.7756 - acc: 0.5913 - val_loss: 1.6939 - val_acc: 0.6479\n",
      "Epoch 13/100\n",
      "9125/9125 [==============================] - 1s 84us/step - loss: 1.7175 - acc: 0.6069 - val_loss: 1.6361 - val_acc: 0.6627\n",
      "Epoch 14/100\n",
      "9125/9125 [==============================] - 1s 109us/step - loss: 1.6603 - acc: 0.6111 - val_loss: 1.5787 - val_acc: 0.6637\n",
      "Epoch 15/100\n",
      "9125/9125 [==============================] - 1s 99us/step - loss: 1.6115 - acc: 0.6110 - val_loss: 1.5279 - val_acc: 0.6647\n",
      "Epoch 16/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 1.5730 - acc: 0.6103 - val_loss: 1.4792 - val_acc: 0.6657\n",
      "Epoch 17/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 1.5269 - acc: 0.6148 - val_loss: 1.4361 - val_acc: 0.6696\n",
      "Epoch 18/100\n",
      "9125/9125 [==============================] - 1s 119us/step - loss: 1.4932 - acc: 0.6127 - val_loss: 1.3946 - val_acc: 0.6726\n",
      "Epoch 19/100\n",
      "9125/9125 [==============================] - 1s 85us/step - loss: 1.4556 - acc: 0.6162 - val_loss: 1.3564 - val_acc: 0.6765\n",
      "Epoch 20/100\n",
      "9125/9125 [==============================] - 1s 80us/step - loss: 1.4187 - acc: 0.6179 - val_loss: 1.3219 - val_acc: 0.6785\n",
      "Epoch 21/100\n",
      "9125/9125 [==============================] - 1s 88us/step - loss: 1.3871 - acc: 0.6170 - val_loss: 1.2810 - val_acc: 0.6805\n",
      "Epoch 22/100\n",
      "9125/9125 [==============================] - 1s 110us/step - loss: 1.3657 - acc: 0.6139 - val_loss: 1.2451 - val_acc: 0.6805\n",
      "Epoch 23/100\n",
      "9125/9125 [==============================] - 1s 75us/step - loss: 1.3245 - acc: 0.6229 - val_loss: 1.2145 - val_acc: 0.6775\n",
      "Epoch 24/100\n",
      "9125/9125 [==============================] - 1s 79us/step - loss: 1.2911 - acc: 0.6233 - val_loss: 1.1870 - val_acc: 0.6726\n",
      "Epoch 25/100\n",
      "9125/9125 [==============================] - 1s 120us/step - loss: 1.2715 - acc: 0.6209 - val_loss: 1.1568 - val_acc: 0.6755\n",
      "Epoch 26/100\n",
      "9125/9125 [==============================] - 1s 84us/step - loss: 1.2447 - acc: 0.6243 - val_loss: 1.1332 - val_acc: 0.6824\n",
      "Epoch 27/100\n",
      "9125/9125 [==============================] - 1s 109us/step - loss: 1.2231 - acc: 0.6256 - val_loss: 1.1045 - val_acc: 0.6805\n",
      "Epoch 28/100\n",
      "9125/9125 [==============================] - 3s 314us/step - loss: 1.1932 - acc: 0.6325 - val_loss: 1.0787 - val_acc: 0.6824\n",
      "Epoch 29/100\n",
      "9125/9125 [==============================] - 1s 84us/step - loss: 1.1789 - acc: 0.6316 - val_loss: 1.0596 - val_acc: 0.6834\n",
      "Epoch 30/100\n",
      "9125/9125 [==============================] - 1s 105us/step - loss: 1.1616 - acc: 0.6279 - val_loss: 1.0339 - val_acc: 0.6834\n",
      "Epoch 31/100\n",
      "9125/9125 [==============================] - 1s 78us/step - loss: 1.1362 - acc: 0.6341 - val_loss: 1.0086 - val_acc: 0.6834\n",
      "Epoch 32/100\n",
      "9125/9125 [==============================] - 1s 69us/step - loss: 1.1244 - acc: 0.6312 - val_loss: 0.9930 - val_acc: 0.6844\n",
      "Epoch 33/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 1.1125 - acc: 0.6301 - val_loss: 0.9722 - val_acc: 0.6844\n",
      "Epoch 34/100\n",
      "9125/9125 [==============================] - 1s 116us/step - loss: 1.0866 - acc: 0.6367 - val_loss: 0.9552 - val_acc: 0.6884\n",
      "Epoch 35/100\n",
      "9125/9125 [==============================] - 1s 79us/step - loss: 1.0779 - acc: 0.6328 - val_loss: 0.9437 - val_acc: 0.6864\n",
      "Epoch 36/100\n",
      "9125/9125 [==============================] - 1s 68us/step - loss: 1.0609 - acc: 0.6364 - val_loss: 0.9259 - val_acc: 0.6874\n",
      "Epoch 37/100\n",
      "9125/9125 [==============================] - 1s 75us/step - loss: 1.0555 - acc: 0.6298 - val_loss: 0.9127 - val_acc: 0.6874\n",
      "Epoch 38/100\n",
      "9125/9125 [==============================] - 1s 109us/step - loss: 1.0260 - acc: 0.6422 - val_loss: 0.9001 - val_acc: 0.6903\n",
      "Epoch 39/100\n",
      "9125/9125 [==============================] - 1s 76us/step - loss: 1.0161 - acc: 0.6421 - val_loss: 0.8836 - val_acc: 0.6923\n",
      "Epoch 40/100\n",
      "9125/9125 [==============================] - 1s 69us/step - loss: 1.0094 - acc: 0.6404 - val_loss: 0.8726 - val_acc: 0.6953\n",
      "Epoch 41/100\n",
      "9125/9125 [==============================] - 1s 74us/step - loss: 0.9912 - acc: 0.6464 - val_loss: 0.8624 - val_acc: 0.6972\n",
      "Epoch 42/100\n",
      "9125/9125 [==============================] - 1s 114us/step - loss: 0.9905 - acc: 0.6431 - val_loss: 0.8489 - val_acc: 0.6982\n",
      "Epoch 43/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 0.9737 - acc: 0.6448 - val_loss: 0.8402 - val_acc: 0.7012\n",
      "Epoch 44/100\n",
      "9125/9125 [==============================] - 1s 71us/step - loss: 0.9611 - acc: 0.6506 - val_loss: 0.8314 - val_acc: 0.7041\n",
      "Epoch 45/100\n",
      "9125/9125 [==============================] - 1s 75us/step - loss: 0.9655 - acc: 0.6493 - val_loss: 0.8206 - val_acc: 0.7051\n",
      "Epoch 46/100\n",
      "9125/9125 [==============================] - 1s 107us/step - loss: 0.9520 - acc: 0.6571 - val_loss: 0.8105 - val_acc: 0.7110\n",
      "Epoch 47/100\n",
      "9125/9125 [==============================] - 1s 75us/step - loss: 0.9368 - acc: 0.6663 - val_loss: 0.8005 - val_acc: 0.7209\n",
      "Epoch 48/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 0.9198 - acc: 0.6713 - val_loss: 0.7893 - val_acc: 0.7268\n",
      "Epoch 49/100\n",
      "9125/9125 [==============================] - 1s 74us/step - loss: 0.9099 - acc: 0.6745 - val_loss: 0.7759 - val_acc: 0.7367\n",
      "Epoch 50/100\n",
      "9125/9125 [==============================] - 1s 119us/step - loss: 0.8989 - acc: 0.6855 - val_loss: 0.7650 - val_acc: 0.7377\n",
      "Epoch 51/100\n",
      "9125/9125 [==============================] - 1s 76us/step - loss: 0.8890 - acc: 0.6843 - val_loss: 0.7579 - val_acc: 0.7465\n",
      "Epoch 52/100\n",
      "9125/9125 [==============================] - 1s 71us/step - loss: 0.8703 - acc: 0.6941 - val_loss: 0.7463 - val_acc: 0.7475\n",
      "Epoch 53/100\n",
      "9125/9125 [==============================] - 1s 100us/step - loss: 0.8703 - acc: 0.6887 - val_loss: 0.7396 - val_acc: 0.7525\n",
      "Epoch 54/100\n",
      "9125/9125 [==============================] - 1s 97us/step - loss: 0.8594 - acc: 0.6916 - val_loss: 0.7280 - val_acc: 0.7515\n",
      "Epoch 55/100\n",
      "9125/9125 [==============================] - 1s 117us/step - loss: 0.8590 - acc: 0.6873 - val_loss: 0.7218 - val_acc: 0.7475\n",
      "Epoch 56/100\n",
      "9125/9125 [==============================] - 2s 201us/step - loss: 0.8424 - acc: 0.6963 - val_loss: 0.7151 - val_acc: 0.7505\n",
      "Epoch 57/100\n",
      "9125/9125 [==============================] - 1s 79us/step - loss: 0.8370 - acc: 0.6958 - val_loss: 0.7059 - val_acc: 0.7475\n",
      "Epoch 58/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 0.8307 - acc: 0.6996 - val_loss: 0.7008 - val_acc: 0.7465\n",
      "Epoch 59/100\n",
      "9125/9125 [==============================] - 1s 115us/step - loss: 0.8184 - acc: 0.7003 - val_loss: 0.6947 - val_acc: 0.7475\n",
      "Epoch 60/100\n",
      "9125/9125 [==============================] - 1s 79us/step - loss: 0.8168 - acc: 0.6987 - val_loss: 0.6873 - val_acc: 0.7525\n",
      "Epoch 61/100\n",
      "9125/9125 [==============================] - 1s 70us/step - loss: 0.8053 - acc: 0.7021 - val_loss: 0.6860 - val_acc: 0.7495\n",
      "Epoch 62/100\n",
      "9125/9125 [==============================] - 1s 68us/step - loss: 0.7953 - acc: 0.7052 - val_loss: 0.6803 - val_acc: 0.7485\n",
      "Epoch 63/100\n",
      "9125/9125 [==============================] - 1s 100us/step - loss: 0.7928 - acc: 0.7078 - val_loss: 0.6722 - val_acc: 0.7505\n",
      "Epoch 64/100\n",
      "9125/9125 [==============================] - 1s 82us/step - loss: 0.7871 - acc: 0.7054 - val_loss: 0.6704 - val_acc: 0.7515\n",
      "Epoch 65/100\n",
      "9125/9125 [==============================] - 1s 70us/step - loss: 0.7912 - acc: 0.7053 - val_loss: 0.6638 - val_acc: 0.7505\n",
      "Epoch 66/100\n",
      "9125/9125 [==============================] - 1s 65us/step - loss: 0.7735 - acc: 0.7099 - val_loss: 0.6582 - val_acc: 0.7525\n",
      "Epoch 67/100\n",
      "9125/9125 [==============================] - 1s 89us/step - loss: 0.7655 - acc: 0.7123 - val_loss: 0.6548 - val_acc: 0.7544\n",
      "Epoch 68/100\n",
      "9125/9125 [==============================] - 1s 93us/step - loss: 0.7625 - acc: 0.7117 - val_loss: 0.6505 - val_acc: 0.7535\n",
      "Epoch 69/100\n",
      "9125/9125 [==============================] - 1s 71us/step - loss: 0.7533 - acc: 0.7139 - val_loss: 0.6463 - val_acc: 0.7564\n",
      "Epoch 70/100\n",
      "9125/9125 [==============================] - 1s 68us/step - loss: 0.7397 - acc: 0.7196 - val_loss: 0.6438 - val_acc: 0.7535\n",
      "Epoch 71/100\n",
      "9125/9125 [==============================] - 1s 79us/step - loss: 0.7491 - acc: 0.7151 - val_loss: 0.6395 - val_acc: 0.7564\n",
      "Epoch 72/100\n",
      "9125/9125 [==============================] - 1s 113us/step - loss: 0.7417 - acc: 0.7182 - val_loss: 0.6343 - val_acc: 0.7564\n",
      "Epoch 73/100\n",
      "9125/9125 [==============================] - 1s 71us/step - loss: 0.7374 - acc: 0.7204 - val_loss: 0.6311 - val_acc: 0.7574\n",
      "Epoch 74/100\n",
      "9125/9125 [==============================] - 1s 75us/step - loss: 0.7273 - acc: 0.7220 - val_loss: 0.6271 - val_acc: 0.7574\n",
      "Epoch 75/100\n",
      "9125/9125 [==============================] - 1s 103us/step - loss: 0.7162 - acc: 0.7253 - val_loss: 0.6259 - val_acc: 0.7604\n",
      "Epoch 76/100\n",
      "9125/9125 [==============================] - 1s 83us/step - loss: 0.7215 - acc: 0.7257 - val_loss: 0.6226 - val_acc: 0.7613\n",
      "Epoch 77/100\n",
      "9125/9125 [==============================] - 1s 73us/step - loss: 0.7220 - acc: 0.7250 - val_loss: 0.6175 - val_acc: 0.7663\n",
      "Epoch 78/100\n",
      "9125/9125 [==============================] - 1s 67us/step - loss: 0.7076 - acc: 0.7363 - val_loss: 0.6150 - val_acc: 0.7712\n",
      "Epoch 79/100\n",
      "9125/9125 [==============================] - 1s 87us/step - loss: 0.7113 - acc: 0.7367 - val_loss: 0.6125 - val_acc: 0.7722\n",
      "Epoch 80/100\n",
      "9125/9125 [==============================] - 1s 103us/step - loss: 0.7090 - acc: 0.7413 - val_loss: 0.6084 - val_acc: 0.7781\n",
      "Epoch 81/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 0.7013 - acc: 0.7402 - val_loss: 0.6054 - val_acc: 0.7801\n",
      "Epoch 82/100\n",
      "9125/9125 [==============================] - 1s 67us/step - loss: 0.6959 - acc: 0.7458 - val_loss: 0.6039 - val_acc: 0.7850\n",
      "Epoch 83/100\n",
      "9125/9125 [==============================] - 1s 104us/step - loss: 0.6904 - acc: 0.7424 - val_loss: 0.6010 - val_acc: 0.7860\n",
      "Epoch 84/100\n",
      "9125/9125 [==============================] - 1s 81us/step - loss: 0.6908 - acc: 0.7510 - val_loss: 0.5982 - val_acc: 0.7870\n",
      "Epoch 85/100\n",
      "9125/9125 [==============================] - 1s 78us/step - loss: 0.6882 - acc: 0.7495 - val_loss: 0.5957 - val_acc: 0.7860\n",
      "Epoch 86/100\n",
      "9125/9125 [==============================] - 1s 69us/step - loss: 0.6752 - acc: 0.7546 - val_loss: 0.5945 - val_acc: 0.7860\n",
      "Epoch 87/100\n",
      "9125/9125 [==============================] - 1s 104us/step - loss: 0.6770 - acc: 0.7523 - val_loss: 0.5916 - val_acc: 0.7860\n",
      "Epoch 88/100\n",
      "9125/9125 [==============================] - 1s 80us/step - loss: 0.6854 - acc: 0.7525 - val_loss: 0.5889 - val_acc: 0.7860\n",
      "Epoch 89/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 0.6702 - acc: 0.7532 - val_loss: 0.5868 - val_acc: 0.7880\n",
      "Epoch 90/100\n",
      "9125/9125 [==============================] - 1s 68us/step - loss: 0.6590 - acc: 0.7611 - val_loss: 0.5848 - val_acc: 0.7870\n",
      "Epoch 91/100\n",
      "9125/9125 [==============================] - 1s 89us/step - loss: 0.6701 - acc: 0.7557 - val_loss: 0.5826 - val_acc: 0.7880\n",
      "Epoch 92/100\n",
      "9125/9125 [==============================] - 1s 96us/step - loss: 0.6679 - acc: 0.7569 - val_loss: 0.5798 - val_acc: 0.7899\n",
      "Epoch 93/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 0.6677 - acc: 0.7581 - val_loss: 0.5778 - val_acc: 0.7870\n",
      "Epoch 94/100\n",
      "9125/9125 [==============================] - 1s 66us/step - loss: 0.6595 - acc: 0.7582 - val_loss: 0.5762 - val_acc: 0.7919\n",
      "Epoch 95/100\n",
      "9125/9125 [==============================] - 1s 88us/step - loss: 0.6568 - acc: 0.7613 - val_loss: 0.5753 - val_acc: 0.7899\n",
      "Epoch 96/100\n",
      "9125/9125 [==============================] - 1s 100us/step - loss: 0.6606 - acc: 0.7575 - val_loss: 0.5738 - val_acc: 0.7909\n",
      "Epoch 97/100\n",
      "9125/9125 [==============================] - 1s 71us/step - loss: 0.6449 - acc: 0.7623 - val_loss: 0.5717 - val_acc: 0.7899\n",
      "Epoch 98/100\n",
      "9125/9125 [==============================] - 1s 69us/step - loss: 0.6450 - acc: 0.7642 - val_loss: 0.5707 - val_acc: 0.7919\n",
      "Epoch 99/100\n",
      "9125/9125 [==============================] - 1s 87us/step - loss: 0.6494 - acc: 0.7642 - val_loss: 0.5698 - val_acc: 0.7899\n",
      "Epoch 100/100\n",
      "9125/9125 [==============================] - 1s 101us/step - loss: 0.6505 - acc: 0.7611 - val_loss: 0.5684 - val_acc: 0.7899\n",
      "(10139, 2161)\n",
      "Train on 9125 samples, validate on 1014 samples\n",
      "Epoch 1/100\n",
      "9125/9125 [==============================] - 5s 523us/step - loss: 2.3764 - acc: 0.4974 - val_loss: 2.3286 - val_acc: 0.7022\n",
      "Epoch 2/100\n",
      "9125/9125 [==============================] - 4s 427us/step - loss: 2.2714 - acc: 0.6612 - val_loss: 2.1513 - val_acc: 0.7613\n",
      "Epoch 3/100\n",
      "9125/9125 [==============================] - 4s 420us/step - loss: 2.1422 - acc: 0.6308 - val_loss: 2.0247 - val_acc: 0.7347\n",
      "Epoch 4/100\n",
      "9125/9125 [==============================] - 4s 455us/step - loss: 2.0223 - acc: 0.6023 - val_loss: 1.8930 - val_acc: 0.6953\n",
      "Epoch 5/100\n",
      "9125/9125 [==============================] - 4s 446us/step - loss: 1.9274 - acc: 0.5995 - val_loss: 1.8089 - val_acc: 0.7012\n",
      "Epoch 6/100\n",
      "9125/9125 [==============================] - 4s 460us/step - loss: 1.8542 - acc: 0.6079 - val_loss: 1.7436 - val_acc: 0.7140\n",
      "Epoch 7/100\n",
      "9125/9125 [==============================] - 4s 474us/step - loss: 1.7980 - acc: 0.6135 - val_loss: 1.6791 - val_acc: 0.7308\n",
      "Epoch 8/100\n",
      "9125/9125 [==============================] - 4s 420us/step - loss: 1.7366 - acc: 0.6026 - val_loss: 1.5974 - val_acc: 0.7032\n",
      "Epoch 9/100\n",
      "9125/9125 [==============================] - 4s 413us/step - loss: 1.6549 - acc: 0.6212 - val_loss: 1.5315 - val_acc: 0.7298\n",
      "Epoch 10/100\n",
      "9125/9125 [==============================] - 4s 459us/step - loss: 1.5953 - acc: 0.6264 - val_loss: 1.4644 - val_acc: 0.7278\n",
      "Epoch 11/100\n",
      "9125/9125 [==============================] - 4s 430us/step - loss: 1.5346 - acc: 0.6329 - val_loss: 1.4109 - val_acc: 0.7288\n",
      "Epoch 12/100\n",
      "9125/9125 [==============================] - 4s 426us/step - loss: 1.4800 - acc: 0.6340 - val_loss: 1.3472 - val_acc: 0.7337\n",
      "Epoch 13/100\n",
      "9125/9125 [==============================] - 4s 458us/step - loss: 1.4303 - acc: 0.6358 - val_loss: 1.2983 - val_acc: 0.7347\n",
      "Epoch 14/100\n",
      "9125/9125 [==============================] - 4s 422us/step - loss: 1.3910 - acc: 0.6402 - val_loss: 1.2512 - val_acc: 0.7337\n",
      "Epoch 15/100\n",
      "9125/9125 [==============================] - 7s 723us/step - loss: 1.3638 - acc: 0.6292 - val_loss: 1.1977 - val_acc: 0.7367\n",
      "Epoch 16/100\n",
      "9125/9125 [==============================] - 4s 481us/step - loss: 1.3171 - acc: 0.6346 - val_loss: 1.1640 - val_acc: 0.7387\n",
      "Epoch 17/100\n",
      "9125/9125 [==============================] - 4s 437us/step - loss: 1.2730 - acc: 0.6421 - val_loss: 1.1246 - val_acc: 0.7416\n",
      "Epoch 18/100\n",
      "9125/9125 [==============================] - 4s 424us/step - loss: 1.2440 - acc: 0.6388 - val_loss: 1.0849 - val_acc: 0.7406\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9125/9125 [==============================] - 4s 432us/step - loss: 1.2055 - acc: 0.6450 - val_loss: 1.0463 - val_acc: 0.7426\n",
      "Epoch 20/100\n",
      "9125/9125 [==============================] - 4s 420us/step - loss: 1.1855 - acc: 0.6412 - val_loss: 1.0136 - val_acc: 0.7406\n",
      "Epoch 21/100\n",
      "9125/9125 [==============================] - 4s 449us/step - loss: 1.1567 - acc: 0.6401 - val_loss: 0.9818 - val_acc: 0.7396\n",
      "Epoch 22/100\n",
      "9125/9125 [==============================] - 4s 436us/step - loss: 1.1214 - acc: 0.6462 - val_loss: 0.9541 - val_acc: 0.7396\n",
      "Epoch 23/100\n",
      "9125/9125 [==============================] - 4s 483us/step - loss: 1.0945 - acc: 0.6503 - val_loss: 0.9288 - val_acc: 0.7377\n",
      "Epoch 24/100\n",
      "9125/9125 [==============================] - 4s 399us/step - loss: 1.0878 - acc: 0.6376 - val_loss: 0.9026 - val_acc: 0.7367\n",
      "Epoch 25/100\n",
      "9125/9125 [==============================] - 4s 432us/step - loss: 1.0638 - acc: 0.6412 - val_loss: 0.8742 - val_acc: 0.7377\n",
      "Epoch 26/100\n",
      "9125/9125 [==============================] - 4s 424us/step - loss: 1.0389 - acc: 0.6460 - val_loss: 0.8469 - val_acc: 0.7357\n",
      "Epoch 27/100\n",
      "9125/9125 [==============================] - 4s 409us/step - loss: 1.0192 - acc: 0.6504 - val_loss: 0.8291 - val_acc: 0.7357\n",
      "Epoch 28/100\n",
      "9125/9125 [==============================] - 4s 403us/step - loss: 1.0001 - acc: 0.6529 - val_loss: 0.8063 - val_acc: 0.7367\n",
      "Epoch 29/100\n",
      "9125/9125 [==============================] - 4s 442us/step - loss: 0.9781 - acc: 0.6530 - val_loss: 0.7910 - val_acc: 0.7396\n",
      "Epoch 30/100\n",
      "9125/9125 [==============================] - 4s 414us/step - loss: 0.9667 - acc: 0.6528 - val_loss: 0.7675 - val_acc: 0.7387\n",
      "Epoch 31/100\n",
      "9125/9125 [==============================] - 4s 428us/step - loss: 0.9632 - acc: 0.6456 - val_loss: 0.7510 - val_acc: 0.7387\n",
      "Epoch 32/100\n",
      "9125/9125 [==============================] - 4s 435us/step - loss: 0.9405 - acc: 0.6523 - val_loss: 0.7411 - val_acc: 0.7485\n",
      "Epoch 33/100\n",
      "9125/9125 [==============================] - 4s 445us/step - loss: 0.9191 - acc: 0.6643 - val_loss: 0.7205 - val_acc: 0.7485\n",
      "Epoch 34/100\n",
      "9125/9125 [==============================] - 4s 409us/step - loss: 0.9040 - acc: 0.6654 - val_loss: 0.7099 - val_acc: 0.7495\n",
      "Epoch 35/100\n",
      "9125/9125 [==============================] - 4s 396us/step - loss: 0.8931 - acc: 0.6665 - val_loss: 0.6974 - val_acc: 0.7515\n",
      "Epoch 36/100\n",
      "9125/9125 [==============================] - 5s 509us/step - loss: 0.8972 - acc: 0.6586 - val_loss: 0.6812 - val_acc: 0.7505\n",
      "Epoch 37/100\n",
      "9125/9125 [==============================] - 4s 403us/step - loss: 0.8729 - acc: 0.6655 - val_loss: 0.6702 - val_acc: 0.7525\n",
      "Epoch 38/100\n",
      "9125/9125 [==============================] - 4s 412us/step - loss: 0.8561 - acc: 0.6716 - val_loss: 0.6594 - val_acc: 0.7535\n",
      "Epoch 39/100\n",
      "9125/9125 [==============================] - 4s 449us/step - loss: 0.8564 - acc: 0.6681 - val_loss: 0.6506 - val_acc: 0.7564\n",
      "Epoch 40/100\n",
      "9125/9125 [==============================] - 4s 420us/step - loss: 0.8563 - acc: 0.6637 - val_loss: 0.6391 - val_acc: 0.7564\n",
      "Epoch 41/100\n",
      "9125/9125 [==============================] - 4s 409us/step - loss: 0.8292 - acc: 0.6719 - val_loss: 0.6319 - val_acc: 0.7584\n",
      "Epoch 42/100\n",
      "9125/9125 [==============================] - 4s 449us/step - loss: 0.8174 - acc: 0.6734 - val_loss: 0.6256 - val_acc: 0.7564\n",
      "Epoch 43/100\n",
      "9125/9125 [==============================] - 4s 436us/step - loss: 0.8225 - acc: 0.6721 - val_loss: 0.6144 - val_acc: 0.7564\n",
      "Epoch 44/100\n",
      "9125/9125 [==============================] - 4s 408us/step - loss: 0.8175 - acc: 0.6752 - val_loss: 0.6091 - val_acc: 0.7574\n",
      "Epoch 45/100\n",
      "9125/9125 [==============================] - 6s 685us/step - loss: 0.7988 - acc: 0.6781 - val_loss: 0.6021 - val_acc: 0.7564\n",
      "Epoch 46/100\n",
      "9125/9125 [==============================] - 5s 494us/step - loss: 0.7954 - acc: 0.6804 - val_loss: 0.5975 - val_acc: 0.7564\n",
      "Epoch 47/100\n",
      "9125/9125 [==============================] - 4s 443us/step - loss: 0.7821 - acc: 0.6849 - val_loss: 0.5901 - val_acc: 0.7574\n",
      "Epoch 48/100\n",
      "9125/9125 [==============================] - 4s 417us/step - loss: 0.7834 - acc: 0.6781 - val_loss: 0.5857 - val_acc: 0.7584\n",
      "Epoch 49/100\n",
      "9125/9125 [==============================] - 4s 488us/step - loss: 0.7800 - acc: 0.6808 - val_loss: 0.5806 - val_acc: 0.7604\n",
      "Epoch 50/100\n",
      "9125/9125 [==============================] - 4s 455us/step - loss: 0.7631 - acc: 0.6868 - val_loss: 0.5763 - val_acc: 0.7574\n",
      "Epoch 51/100\n",
      "9125/9125 [==============================] - 4s 466us/step - loss: 0.7627 - acc: 0.6841 - val_loss: 0.5724 - val_acc: 0.7584\n",
      "Epoch 52/100\n",
      "9125/9125 [==============================] - 4s 440us/step - loss: 0.7669 - acc: 0.6818 - val_loss: 0.5677 - val_acc: 0.7604\n",
      "Epoch 53/100\n",
      "9125/9125 [==============================] - 4s 415us/step - loss: 0.7571 - acc: 0.6876 - val_loss: 0.5634 - val_acc: 0.7574\n",
      "Epoch 54/100\n",
      "9125/9125 [==============================] - 4s 456us/step - loss: 0.7458 - acc: 0.6887 - val_loss: 0.5607 - val_acc: 0.7594\n",
      "Epoch 55/100\n",
      "9125/9125 [==============================] - 4s 423us/step - loss: 0.7547 - acc: 0.6803 - val_loss: 0.5555 - val_acc: 0.7594\n",
      "Epoch 56/100\n",
      "9125/9125 [==============================] - 4s 411us/step - loss: 0.7423 - acc: 0.6896 - val_loss: 0.5542 - val_acc: 0.7584\n",
      "Epoch 57/100\n",
      "9125/9125 [==============================] - 4s 474us/step - loss: 0.7266 - acc: 0.6921 - val_loss: 0.5516 - val_acc: 0.7574\n",
      "Epoch 58/100\n",
      "9125/9125 [==============================] - 4s 450us/step - loss: 0.7195 - acc: 0.6948 - val_loss: 0.5508 - val_acc: 0.7574\n",
      "Epoch 59/100\n",
      "9125/9125 [==============================] - 4s 419us/step - loss: 0.7355 - acc: 0.6870 - val_loss: 0.5463 - val_acc: 0.7594\n",
      "Epoch 60/100\n",
      "9125/9125 [==============================] - 4s 466us/step - loss: 0.7275 - acc: 0.6908 - val_loss: 0.5434 - val_acc: 0.7673\n",
      "Epoch 61/100\n",
      "9125/9125 [==============================] - 4s 451us/step - loss: 0.7168 - acc: 0.6921 - val_loss: 0.5410 - val_acc: 0.7682\n",
      "Epoch 62/100\n",
      "9125/9125 [==============================] - 4s 419us/step - loss: 0.7208 - acc: 0.6934 - val_loss: 0.5405 - val_acc: 0.7702\n",
      "Epoch 63/100\n",
      "9125/9125 [==============================] - 4s 481us/step - loss: 0.7024 - acc: 0.6963 - val_loss: 0.5374 - val_acc: 0.7692\n",
      "Epoch 64/100\n",
      "9125/9125 [==============================] - 4s 423us/step - loss: 0.7011 - acc: 0.6984 - val_loss: 0.5356 - val_acc: 0.7692\n",
      "Epoch 65/100\n",
      "9125/9125 [==============================] - 4s 480us/step - loss: 0.7001 - acc: 0.7016 - val_loss: 0.5340 - val_acc: 0.7692\n",
      "Epoch 66/100\n",
      "9125/9125 [==============================] - 4s 488us/step - loss: 0.7021 - acc: 0.6985 - val_loss: 0.5345 - val_acc: 0.7682\n",
      "Epoch 67/100\n",
      "9125/9125 [==============================] - 5s 577us/step - loss: 0.6907 - acc: 0.7027 - val_loss: 0.5337 - val_acc: 0.7682\n",
      "Epoch 68/100\n",
      "9125/9125 [==============================] - 4s 424us/step - loss: 0.6861 - acc: 0.7064 - val_loss: 0.5338 - val_acc: 0.7682\n",
      "Epoch 69/100\n",
      "9125/9125 [==============================] - 4s 417us/step - loss: 0.6871 - acc: 0.7041 - val_loss: 0.5317 - val_acc: 0.7692\n",
      "Epoch 70/100\n",
      "9125/9125 [==============================] - 4s 458us/step - loss: 0.6797 - acc: 0.7054 - val_loss: 0.5295 - val_acc: 0.7692\n",
      "Epoch 71/100\n",
      "9125/9125 [==============================] - 4s 439us/step - loss: 0.6712 - acc: 0.7089 - val_loss: 0.5289 - val_acc: 0.7692\n",
      "Epoch 72/100\n",
      "9125/9125 [==============================] - 4s 414us/step - loss: 0.6706 - acc: 0.7085 - val_loss: 0.5291 - val_acc: 0.7692\n",
      "Epoch 73/100\n",
      "9125/9125 [==============================] - 4s 478us/step - loss: 0.6645 - acc: 0.7122 - val_loss: 0.5300 - val_acc: 0.7692\n",
      "Epoch 74/100\n",
      "9125/9125 [==============================] - 7s 749us/step - loss: 0.6576 - acc: 0.7135 - val_loss: 0.5278 - val_acc: 0.7692\n",
      "Epoch 75/100\n",
      "9125/9125 [==============================] - 4s 431us/step - loss: 0.6529 - acc: 0.7168 - val_loss: 0.5270 - val_acc: 0.7692\n",
      "Epoch 76/100\n",
      "9125/9125 [==============================] - 4s 474us/step - loss: 0.6483 - acc: 0.7197 - val_loss: 0.5264 - val_acc: 0.7692\n",
      "Epoch 77/100\n",
      "9125/9125 [==============================] - 4s 424us/step - loss: 0.6589 - acc: 0.7110 - val_loss: 0.5248 - val_acc: 0.7692\n",
      "Epoch 78/100\n",
      "9125/9125 [==============================] - 4s 406us/step - loss: 0.6470 - acc: 0.7180 - val_loss: 0.5251 - val_acc: 0.7702\n",
      "Epoch 79/100\n",
      "9125/9125 [==============================] - 4s 486us/step - loss: 0.6398 - acc: 0.7222 - val_loss: 0.5255 - val_acc: 0.7692\n",
      "Epoch 80/100\n",
      "9125/9125 [==============================] - 4s 414us/step - loss: 0.6356 - acc: 0.7268 - val_loss: 0.5253 - val_acc: 0.7702\n",
      "Epoch 81/100\n",
      "9125/9125 [==============================] - 4s 401us/step - loss: 0.6325 - acc: 0.7299 - val_loss: 0.5231 - val_acc: 0.7712\n",
      "Epoch 82/100\n",
      "9125/9125 [==============================] - 4s 440us/step - loss: 0.6392 - acc: 0.7292 - val_loss: 0.5223 - val_acc: 0.7712\n",
      "Epoch 83/100\n",
      "9125/9125 [==============================] - 4s 413us/step - loss: 0.6196 - acc: 0.7339 - val_loss: 0.5208 - val_acc: 0.7712\n",
      "Epoch 84/100\n",
      "9125/9125 [==============================] - 4s 416us/step - loss: 0.6192 - acc: 0.7369 - val_loss: 0.5225 - val_acc: 0.7712\n",
      "Epoch 85/100\n",
      "9125/9125 [==============================] - 4s 410us/step - loss: 0.6114 - acc: 0.7397 - val_loss: 0.5215 - val_acc: 0.7742\n",
      "Epoch 86/100\n",
      "9125/9125 [==============================] - 4s 448us/step - loss: 0.6097 - acc: 0.7379 - val_loss: 0.5199 - val_acc: 0.7742\n",
      "Epoch 87/100\n",
      "9125/9125 [==============================] - 4s 415us/step - loss: 0.6065 - acc: 0.7425 - val_loss: 0.5175 - val_acc: 0.7742\n",
      "Epoch 88/100\n",
      "9125/9125 [==============================] - 4s 413us/step - loss: 0.5968 - acc: 0.7419 - val_loss: 0.5186 - val_acc: 0.7761\n",
      "Epoch 89/100\n",
      "9125/9125 [==============================] - 4s 438us/step - loss: 0.6053 - acc: 0.7440 - val_loss: 0.5187 - val_acc: 0.7761\n",
      "Epoch 90/100\n",
      "9125/9125 [==============================] - 4s 428us/step - loss: 0.5983 - acc: 0.7462 - val_loss: 0.5192 - val_acc: 0.7751\n",
      "Epoch 91/100\n",
      "9125/9125 [==============================] - 4s 410us/step - loss: 0.5829 - acc: 0.7512 - val_loss: 0.5185 - val_acc: 0.7742\n",
      "Epoch 92/100\n",
      "9125/9125 [==============================] - 4s 413us/step - loss: 0.5877 - acc: 0.7532 - val_loss: 0.5201 - val_acc: 0.7751\n",
      "Epoch 93/100\n",
      "9125/9125 [==============================] - 4s 446us/step - loss: 0.5832 - acc: 0.7521 - val_loss: 0.5176 - val_acc: 0.7751\n",
      "Epoch 94/100\n",
      "9125/9125 [==============================] - 4s 463us/step - loss: 0.5750 - acc: 0.7525 - val_loss: 0.5198 - val_acc: 0.7771\n",
      "Epoch 95/100\n",
      "9125/9125 [==============================] - 4s 431us/step - loss: 0.5788 - acc: 0.7535 - val_loss: 0.5239 - val_acc: 0.7781\n",
      "Epoch 96/100\n",
      "9125/9125 [==============================] - 5s 546us/step - loss: 0.5738 - acc: 0.7558 - val_loss: 0.5221 - val_acc: 0.7771\n",
      "Epoch 97/100\n",
      "9125/9125 [==============================] - 8s 860us/step - loss: 0.5615 - acc: 0.7588 - val_loss: 0.5207 - val_acc: 0.7791\n",
      "Epoch 98/100\n",
      "9125/9125 [==============================] - 7s 747us/step - loss: 0.5689 - acc: 0.7667 - val_loss: 0.5223 - val_acc: 0.7761\n",
      "Epoch 99/100\n",
      "9125/9125 [==============================] - 4s 452us/step - loss: 0.5630 - acc: 0.7641 - val_loss: 0.5227 - val_acc: 0.7761\n",
      "Epoch 100/100\n",
      "9125/9125 [==============================] - 4s 448us/step - loss: 0.5544 - acc: 0.7707 - val_loss: 0.5215 - val_acc: 0.7751\n",
      "(10139, 21082)\n",
      "Train on 9125 samples, validate on 1014 samples\n",
      "Epoch 1/100\n",
      "9125/9125 [==============================] - 43s 5ms/step - loss: 2.3430 - acc: 0.6861 - val_loss: 2.1832 - val_acc: 0.7110\n",
      "Epoch 2/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 2.1707 - acc: 0.5938 - val_loss: 2.0433 - val_acc: 0.6519\n",
      "Epoch 3/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 2.0469 - acc: 0.5855 - val_loss: 1.9369 - val_acc: 0.6864\n",
      "Epoch 4/100\n",
      "9125/9125 [==============================] - 41s 5ms/step - loss: 1.9589 - acc: 0.5907 - val_loss: 1.8619 - val_acc: 0.6874\n",
      "Epoch 5/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 1.8918 - acc: 0.5934 - val_loss: 1.8047 - val_acc: 0.6884\n",
      "Epoch 6/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 1.8324 - acc: 0.5979 - val_loss: 1.7516 - val_acc: 0.6884\n",
      "Epoch 7/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 1.7811 - acc: 0.5965 - val_loss: 1.6924 - val_acc: 0.6884\n",
      "Epoch 8/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 1.7258 - acc: 0.5978 - val_loss: 1.6338 - val_acc: 0.6874\n",
      "Epoch 9/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 1.6719 - acc: 0.5976 - val_loss: 1.5817 - val_acc: 0.6864\n",
      "Epoch 10/100\n",
      "9125/9125 [==============================] - 41s 5ms/step - loss: 1.6230 - acc: 0.6020 - val_loss: 1.5394 - val_acc: 0.6864\n",
      "Epoch 11/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 1.5801 - acc: 0.6027 - val_loss: 1.4852 - val_acc: 0.6874\n",
      "Epoch 12/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 1.5324 - acc: 0.6072 - val_loss: 1.4500 - val_acc: 0.6874\n",
      "Epoch 13/100\n",
      "9125/9125 [==============================] - 41s 4ms/step - loss: 1.5010 - acc: 0.6033 - val_loss: 1.4134 - val_acc: 0.6874\n",
      "Epoch 14/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 1.4663 - acc: 0.5993 - val_loss: 1.3648 - val_acc: 0.6874\n",
      "Epoch 15/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 1.4337 - acc: 0.6025 - val_loss: 1.3315 - val_acc: 0.6874\n",
      "Epoch 16/100\n",
      "9125/9125 [==============================] - 41s 5ms/step - loss: 1.3989 - acc: 0.6045 - val_loss: 1.2909 - val_acc: 0.6874\n",
      "Epoch 17/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 1.3595 - acc: 0.6088 - val_loss: 1.2624 - val_acc: 0.6874\n",
      "Epoch 18/100\n",
      "9125/9125 [==============================] - 43s 5ms/step - loss: 1.3398 - acc: 0.6041 - val_loss: 1.2196 - val_acc: 0.6874\n",
      "Epoch 19/100\n",
      "9125/9125 [==============================] - 48s 5ms/step - loss: 1.3057 - acc: 0.6069 - val_loss: 1.1942 - val_acc: 0.6874\n",
      "Epoch 20/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 1.2767 - acc: 0.6089 - val_loss: 1.1654 - val_acc: 0.6874\n",
      "Epoch 21/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 1.2455 - acc: 0.6111 - val_loss: 1.1375 - val_acc: 0.6874\n",
      "Epoch 22/100\n",
      "9125/9125 [==============================] - 37s 4ms/step - loss: 1.2376 - acc: 0.6081 - val_loss: 1.1123 - val_acc: 0.6874\n",
      "Epoch 23/100\n",
      "9125/9125 [==============================] - 34s 4ms/step - loss: 1.2014 - acc: 0.6164 - val_loss: 1.0781 - val_acc: 0.6893\n",
      "Epoch 24/100\n",
      "9125/9125 [==============================] - 36s 4ms/step - loss: 1.1850 - acc: 0.6114 - val_loss: 1.0578 - val_acc: 0.6893\n",
      "Epoch 25/100\n",
      "9125/9125 [==============================] - 41s 5ms/step - loss: 1.1627 - acc: 0.6142 - val_loss: 1.0365 - val_acc: 0.6893\n",
      "Epoch 26/100\n",
      "9125/9125 [==============================] - 37s 4ms/step - loss: 1.1347 - acc: 0.6185 - val_loss: 1.0198 - val_acc: 0.6893\n",
      "Epoch 27/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 1.1353 - acc: 0.6124 - val_loss: 0.9939 - val_acc: 0.6893\n",
      "Epoch 28/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 1.0993 - acc: 0.6175 - val_loss: 0.9818 - val_acc: 0.6893\n",
      "Epoch 29/100\n",
      "9125/9125 [==============================] - 37s 4ms/step - loss: 1.0813 - acc: 0.6192 - val_loss: 0.9622 - val_acc: 0.6884\n",
      "Epoch 30/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 1.0826 - acc: 0.6146 - val_loss: 0.9400 - val_acc: 0.6874\n",
      "Epoch 31/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 1.0655 - acc: 0.6197 - val_loss: 0.9224 - val_acc: 0.6884\n",
      "Epoch 32/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 1.0382 - acc: 0.6230 - val_loss: 0.9092 - val_acc: 0.6874\n",
      "Epoch 33/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 1.0331 - acc: 0.6245 - val_loss: 0.8943 - val_acc: 0.6864\n",
      "Epoch 34/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 1.0293 - acc: 0.6247 - val_loss: 0.8799 - val_acc: 0.6874\n",
      "Epoch 35/100\n",
      "9125/9125 [==============================] - 37s 4ms/step - loss: 1.0094 - acc: 0.6256 - val_loss: 0.8716 - val_acc: 0.6864\n",
      "Epoch 36/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 0.9999 - acc: 0.6289 - val_loss: 0.8595 - val_acc: 0.6874\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9125/9125 [==============================] - 38s 4ms/step - loss: 0.9890 - acc: 0.6283 - val_loss: 0.8459 - val_acc: 0.6874\n",
      "Epoch 38/100\n",
      "9125/9125 [==============================] - 36s 4ms/step - loss: 0.9684 - acc: 0.6302 - val_loss: 0.8334 - val_acc: 0.6874\n",
      "Epoch 39/100\n",
      "9125/9125 [==============================] - 36s 4ms/step - loss: 0.9759 - acc: 0.6238 - val_loss: 0.8238 - val_acc: 0.6874\n",
      "Epoch 40/100\n",
      "9125/9125 [==============================] - 36s 4ms/step - loss: 0.9650 - acc: 0.6307 - val_loss: 0.8167 - val_acc: 0.6874\n",
      "Epoch 41/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 0.9600 - acc: 0.6342 - val_loss: 0.8066 - val_acc: 0.6864\n",
      "Epoch 42/100\n",
      "9125/9125 [==============================] - 37s 4ms/step - loss: 0.9463 - acc: 0.6327 - val_loss: 0.8013 - val_acc: 0.6874\n",
      "Epoch 43/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 0.9326 - acc: 0.6384 - val_loss: 0.7920 - val_acc: 0.6874\n",
      "Epoch 44/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.9173 - acc: 0.6411 - val_loss: 0.7891 - val_acc: 0.6874\n",
      "Epoch 45/100\n",
      "9125/9125 [==============================] - 37s 4ms/step - loss: 0.9107 - acc: 0.6401 - val_loss: 0.7797 - val_acc: 0.6874\n",
      "Epoch 46/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.9164 - acc: 0.6357 - val_loss: 0.7736 - val_acc: 0.6874\n",
      "Epoch 47/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.9067 - acc: 0.6355 - val_loss: 0.7679 - val_acc: 0.6874\n",
      "Epoch 48/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 0.9008 - acc: 0.6368 - val_loss: 0.7663 - val_acc: 0.6874\n",
      "Epoch 49/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.8857 - acc: 0.6413 - val_loss: 0.7612 - val_acc: 0.6884\n",
      "Epoch 50/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.8907 - acc: 0.6395 - val_loss: 0.7564 - val_acc: 0.6884\n",
      "Epoch 51/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.8771 - acc: 0.6430 - val_loss: 0.7533 - val_acc: 0.6884\n",
      "Epoch 52/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.8743 - acc: 0.6405 - val_loss: 0.7491 - val_acc: 0.6874\n",
      "Epoch 53/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.8641 - acc: 0.6445 - val_loss: 0.7453 - val_acc: 0.6982\n",
      "Epoch 54/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.8650 - acc: 0.6391 - val_loss: 0.7390 - val_acc: 0.6893\n",
      "Epoch 55/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.8616 - acc: 0.6401 - val_loss: 0.7387 - val_acc: 0.6893\n",
      "Epoch 56/100\n",
      "9125/9125 [==============================] - 43s 5ms/step - loss: 0.8445 - acc: 0.6483 - val_loss: 0.7392 - val_acc: 0.6992\n",
      "Epoch 57/100\n",
      "9125/9125 [==============================] - 41s 4ms/step - loss: 0.8446 - acc: 0.6460 - val_loss: 0.7343 - val_acc: 0.6903\n",
      "Epoch 58/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.8399 - acc: 0.6434 - val_loss: 0.7357 - val_acc: 0.7012\n",
      "Epoch 59/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.8429 - acc: 0.6404 - val_loss: 0.7314 - val_acc: 0.6933\n",
      "Epoch 60/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.8178 - acc: 0.6513 - val_loss: 0.7273 - val_acc: 0.7022\n",
      "Epoch 61/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.8060 - acc: 0.6505 - val_loss: 0.7271 - val_acc: 0.7022\n",
      "Epoch 62/100\n",
      "9125/9125 [==============================] - 41s 5ms/step - loss: 0.8057 - acc: 0.6505 - val_loss: 0.7220 - val_acc: 0.7041\n",
      "Epoch 63/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.7953 - acc: 0.6490 - val_loss: 0.7205 - val_acc: 0.7071\n",
      "Epoch 64/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.7976 - acc: 0.6502 - val_loss: 0.7151 - val_acc: 0.7071\n",
      "Epoch 65/100\n",
      "9125/9125 [==============================] - 43s 5ms/step - loss: 0.7940 - acc: 0.6495 - val_loss: 0.7135 - val_acc: 0.7051\n",
      "Epoch 66/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.7783 - acc: 0.6568 - val_loss: 0.7167 - val_acc: 0.7061\n",
      "Epoch 67/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.7668 - acc: 0.6585 - val_loss: 0.7082 - val_acc: 0.7061\n",
      "Epoch 68/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.7678 - acc: 0.6556 - val_loss: 0.7062 - val_acc: 0.7051\n",
      "Epoch 69/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.7573 - acc: 0.6635 - val_loss: 0.7053 - val_acc: 0.7061\n",
      "Epoch 70/100\n",
      "9125/9125 [==============================] - 50s 5ms/step - loss: 0.7473 - acc: 0.6654 - val_loss: 0.7056 - val_acc: 0.7091\n",
      "Epoch 71/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.7513 - acc: 0.6613 - val_loss: 0.7038 - val_acc: 0.7101\n",
      "Epoch 72/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.7491 - acc: 0.6671 - val_loss: 0.7026 - val_acc: 0.7101\n",
      "Epoch 73/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.7416 - acc: 0.6683 - val_loss: 0.6993 - val_acc: 0.7150\n",
      "Epoch 74/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.7433 - acc: 0.6666 - val_loss: 0.6979 - val_acc: 0.7160\n",
      "Epoch 75/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.7376 - acc: 0.6712 - val_loss: 0.6970 - val_acc: 0.7179\n",
      "Epoch 76/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.7242 - acc: 0.6770 - val_loss: 0.6931 - val_acc: 0.7160\n",
      "Epoch 77/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.7177 - acc: 0.6785 - val_loss: 0.6919 - val_acc: 0.7170\n",
      "Epoch 78/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.7281 - acc: 0.6729 - val_loss: 0.6960 - val_acc: 0.7179\n",
      "Epoch 79/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.7213 - acc: 0.6797 - val_loss: 0.6942 - val_acc: 0.7189\n",
      "Epoch 80/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.7153 - acc: 0.6799 - val_loss: 0.6893 - val_acc: 0.7239\n",
      "Epoch 81/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.7154 - acc: 0.6805 - val_loss: 0.6913 - val_acc: 0.7249\n",
      "Epoch 82/100\n",
      "9125/9125 [==============================] - 44s 5ms/step - loss: 0.7139 - acc: 0.6845 - val_loss: 0.6891 - val_acc: 0.7219\n",
      "Epoch 83/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.7053 - acc: 0.6858 - val_loss: 0.6898 - val_acc: 0.7239\n",
      "Epoch 84/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.6983 - acc: 0.6941 - val_loss: 0.6847 - val_acc: 0.7239\n",
      "Epoch 85/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.6921 - acc: 0.6883 - val_loss: 0.6838 - val_acc: 0.7239\n",
      "Epoch 86/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.6943 - acc: 0.6940 - val_loss: 0.6899 - val_acc: 0.7239\n",
      "Epoch 87/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.6841 - acc: 0.6971 - val_loss: 0.6893 - val_acc: 0.7229\n",
      "Epoch 88/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.6777 - acc: 0.6973 - val_loss: 0.6856 - val_acc: 0.7249\n",
      "Epoch 89/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.6806 - acc: 0.6970 - val_loss: 0.6843 - val_acc: 0.7278\n",
      "Epoch 90/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.6757 - acc: 0.7010 - val_loss: 0.6831 - val_acc: 0.7268\n",
      "Epoch 91/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.6751 - acc: 0.6998 - val_loss: 0.6840 - val_acc: 0.7268\n",
      "Epoch 92/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.6669 - acc: 0.7022 - val_loss: 0.6785 - val_acc: 0.7278\n",
      "Epoch 93/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.6703 - acc: 0.6988 - val_loss: 0.6817 - val_acc: 0.7318\n",
      "Epoch 94/100\n",
      "9125/9125 [==============================] - 43s 5ms/step - loss: 0.6584 - acc: 0.7030 - val_loss: 0.6785 - val_acc: 0.7327\n",
      "Epoch 95/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.6591 - acc: 0.7048 - val_loss: 0.6796 - val_acc: 0.7308\n",
      "Epoch 96/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.6533 - acc: 0.7038 - val_loss: 0.6775 - val_acc: 0.7318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      "9125/9125 [==============================] - 48s 5ms/step - loss: 0.6554 - acc: 0.7082 - val_loss: 0.6791 - val_acc: 0.7347\n",
      "Epoch 98/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 0.6547 - acc: 0.7054 - val_loss: 0.6738 - val_acc: 0.7357\n",
      "Epoch 99/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 0.6434 - acc: 0.7110 - val_loss: 0.6735 - val_acc: 0.7387\n",
      "Epoch 100/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.6421 - acc: 0.7144 - val_loss: 0.6731 - val_acc: 0.7367\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in training_data:\n",
    "    print(i['document_matrix'].shape)\n",
    "    model = simple_ffn(i['document_matrix'], i['labels'])\n",
    "    model.fit(i['document_matrix'], i['labels'], epochs=100, validation_split=0.1)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(os.path.join(MODELS_PATH,'ffn_sample_model_sentences_bi.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on unseen data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for i in training_data:\n",
    "    X_test, y_test = prepare_test_data(corpora_test, labels_test, i['pipeline_instance'])\n",
    "    test_data.append({'X_test': X_test, 'y_test': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>\"</th>\n",
       "      <th>#</th>\n",
       "      <th>$</th>\n",
       "      <th>%</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>'</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>...</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>w</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>{</th>\n",
       "      <th>|</th>\n",
       "      <th>}</th>\n",
       "      <th>~</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.351427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080407</td>\n",
       "      <td>0.080444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.168340</td>\n",
       "      <td>0.118161</td>\n",
       "      <td>0.178917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.531607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038198</td>\n",
       "      <td>0.268115</td>\n",
       "      <td>0.121792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038955</td>\n",
       "      <td>0.054692</td>\n",
       "      <td>0.066025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.231594</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188653</td>\n",
       "      <td>0.151879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.380765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075985</td>\n",
       "      <td>0.065340</td>\n",
       "      <td>0.065370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336291</td>\n",
       "      <td>0.018886</td>\n",
       "      <td>0.054718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087234</td>\n",
       "      <td>0.070230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               !    \"    #    $    %    &         '         (         ) ...   \\\n",
       "0  0.351427  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.080407  0.080444 ...    \n",
       "1  0.531607  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000 ...    \n",
       "2  0.470930  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000 ...    \n",
       "3  0.231594  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000 ...    \n",
       "4  0.380765  0.0  0.0  0.0  0.0  0.0  0.0  0.075985  0.065340  0.065370 ...    \n",
       "\n",
       "          u         v         w         x         y         z    {    |    }  \\\n",
       "0  0.198643  0.000000  0.168340  0.118161  0.178917  0.000000  0.0  0.0  0.0   \n",
       "1  0.037561  0.000000  0.038198  0.268115  0.121792  0.000000  0.0  0.0  0.0   \n",
       "2  0.038955  0.054692  0.066025  0.000000  0.014035  0.000000  0.0  0.0  0.0   \n",
       "3  0.087272  0.000000  0.266251  0.000000  0.188653  0.151879  0.0  0.0  0.0   \n",
       "4  0.336291  0.018886  0.054718  0.000000  0.087234  0.070230  0.0  0.0  0.0   \n",
       "\n",
       "     ~  \n",
       "0  0.0  \n",
       "1  0.0  \n",
       "2  0.0  \n",
       "3  0.0  \n",
       "4  0.0  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]['X_test'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>af</th>\n",
       "      <th>en</th>\n",
       "      <th>nr</th>\n",
       "      <th>nso</th>\n",
       "      <th>ss</th>\n",
       "      <th>st</th>\n",
       "      <th>tn</th>\n",
       "      <th>ts</th>\n",
       "      <th>ve</th>\n",
       "      <th>xh</th>\n",
       "      <th>zu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34750</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18986</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13655</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15126</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21978</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       af  en  nr  nso  ss  st  tn  ts  ve  xh  zu\n",
       "34750   0   0   0    0   0   0   0   0   0   1   0\n",
       "18986   0   0   0    1   0   0   0   0   0   0   0\n",
       "13655   0   1   0    0   0   0   0   0   0   0   0\n",
       "15126   0   0   1    0   0   0   0   0   0   0   0\n",
       "21978   0   0   0    0   1   0   0   0   0   0   0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]['y_test'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30417/30417 [==============================] - 1s 28us/step\n",
      "Model test accuracy 77.2\n",
      "30417/30417 [==============================] - 2s 51us/step\n",
      "Model test accuracy 75.77000000000001\n",
      "30417/30417 [==============================] - 15s 504us/step\n",
      "Model test accuracy 71.95\n"
     ]
    }
   ],
   "source": [
    "for idx, i in enumerate(test_data):\n",
    "    score, accuracy = models[idx].evaluate(i['X_test'], i['y_test'])\n",
    "    print('Model test accuracy', accuracy.round(4)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sa_lang",
   "language": "python",
   "name": "sa_lang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
