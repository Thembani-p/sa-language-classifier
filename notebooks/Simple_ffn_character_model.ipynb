{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple feed forward model\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "- [Read data](#Read-data)\n",
    "- [Prepare data](#Prepare-data)\n",
    "- [Create and train model](#Create-and-train-model)\n",
    "- [Test on unseen data](#Test-on-unseen-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "MODULES_PATH = '../modules'\n",
    "MODELS_PATH = '../models'\n",
    "DATA_PATH = '../data'\n",
    "\n",
    "sys.path.append(MODULES_PATH)\n",
    "from data import flatten_data, prepare_training_data, prepare_test_data, \\\n",
    "                    raise_one_level\n",
    "from models import simple_ffn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH,'sentences.json'),'r') as datafile:\n",
    "    sentences = json.load(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = pd.read_csv(os.path.join(DATA_PATH,'training_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_flat = raise_one_level(sentences)\n",
    "sentences_df = pd.DataFrame(sentences_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flat_corpora, flat_labels = flatten_data(single_corpora[5:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpora_train, corpora_test, labels_train, labels_test = train_test_split(\n",
    "                                                        sentences_df['body'],\n",
    "                                                        sentences_df['class'],\n",
    "                                                        test_size=0.75,\n",
    "                                                        random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "training_data = []\n",
    "for i in range(1,4):\n",
    "    print(i+1)\n",
    "\n",
    "    document_matrix, labels, pipeline_instance = prepare_training_data(corpora_train, labels_train, (i,i))\n",
    "    training_data.append({'document_matrix': document_matrix, 'labels': labels, 'pipeline_instance': pipeline_instance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10139, 68), (10139, 11), Pipeline(memory=None,\n",
       "      steps=[('vect', CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]['document_matrix'].shape, training_data[0]['labels'].shape, training_data[0]['pipeline_instance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10139, 68)\n",
      "(10139, 2161)\n",
      "(10139, 21082)\n"
     ]
    }
   ],
   "source": [
    "for i in training_data:\n",
    "    print(i['document_matrix'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>0.435572</td>\n",
       "      <td>0.368490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571279</td>\n",
       "      <td>0.404572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"</th>\n",
       "      <td>0.128529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>$</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&amp;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028501</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>*</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>+</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027372</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.212986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.140612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>:</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>=</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.534686</td>\n",
       "      <td>0.370096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.367571</td>\n",
       "      <td>0.352157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0.107877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059690</td>\n",
       "      <td>0.030060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010326</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0.146546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153163</td>\n",
       "      <td>0.381126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059414</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>0.108801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040134</td>\n",
       "      <td>0.090951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>0.104526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096393</td>\n",
       "      <td>0.174756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.248608</td>\n",
       "      <td>0.378576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458528</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <td>0.057237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.263919</td>\n",
       "      <td>0.063796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l</th>\n",
       "      <td>0.050256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064884</td>\n",
       "      <td>0.336094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>0.210663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.077708</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>0.295450</td>\n",
       "      <td>0.374921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163477</td>\n",
       "      <td>0.109769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o</th>\n",
       "      <td>0.099345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100776</td>\n",
       "      <td>0.332188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>0.061784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022791</td>\n",
       "      <td>0.068865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.115550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117215</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>0.102609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.170326</td>\n",
       "      <td>0.085776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>0.050823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.112484</td>\n",
       "      <td>0.113294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u</th>\n",
       "      <td>0.054712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.191730</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v</th>\n",
       "      <td>0.076816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028336</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w</th>\n",
       "      <td>0.055639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133406</td>\n",
       "      <td>0.031008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>0.292907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.180077</td>\n",
       "      <td>0.326472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.152695</td>\n",
       "      <td>0.032956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017561</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>|</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>}</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>~</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1    2         3         4\n",
       "    0.435572  0.368490  0.0  0.571279  0.404572\n",
       "!   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "\"   0.128529  0.000000  0.0  0.000000  0.000000\n",
       "#   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "$   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "%   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "&   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "'   0.000000  0.000000  0.0  0.028501  0.000000\n",
       "(   0.000000  0.000000  0.0  0.000000  0.000000\n",
       ")   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "*   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "+   0.000000  0.000000  0.0  0.000000  0.000000\n",
       ",   0.000000  0.000000  0.0  0.027372  0.000000\n",
       "-   0.000000  0.000000  0.0  0.000000  0.000000\n",
       ".   0.000000  0.000000  1.0  0.000000  0.000000\n",
       "/   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "0   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "1   0.212986  0.000000  0.0  0.000000  0.178045\n",
       "2   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "3   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "4   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "5   0.000000  0.000000  0.0  0.000000  0.211784\n",
       "6   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "7   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "8   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "9   0.140612  0.000000  0.0  0.000000  0.000000\n",
       ":   0.000000  0.000000  0.0  0.000000  0.000000\n",
       ";   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "<   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "=   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "..       ...       ...  ...       ...       ...\n",
       "a   0.534686  0.370096  0.0  0.367571  0.352157\n",
       "b   0.107877  0.000000  0.0  0.059690  0.030060\n",
       "c   0.000000  0.000000  0.0  0.000000  0.126934\n",
       "d   0.000000  0.000000  0.0  0.010326  0.000000\n",
       "e   0.146546  0.000000  0.0  0.153163  0.381126\n",
       "f   0.000000  0.490545  0.0  0.059414  0.000000\n",
       "g   0.108801  0.000000  0.0  0.040134  0.090951\n",
       "h   0.104526  0.000000  0.0  0.096393  0.174756\n",
       "i   0.248608  0.378576  0.0  0.458528  0.000000\n",
       "j   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "k   0.057237  0.000000  0.0  0.263919  0.063796\n",
       "l   0.050256  0.000000  0.0  0.064884  0.336094\n",
       "m   0.210663  0.000000  0.0  0.077708  0.000000\n",
       "n   0.295450  0.374921  0.0  0.163477  0.109769\n",
       "o   0.099345  0.000000  0.0  0.100776  0.332188\n",
       "p   0.061784  0.000000  0.0  0.022791  0.068865\n",
       "q   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "r   0.115550  0.000000  0.0  0.117215  0.000000\n",
       "s   0.102609  0.000000  0.0  0.170326  0.085776\n",
       "t   0.050823  0.000000  0.0  0.112484  0.113294\n",
       "u   0.054712  0.000000  0.0  0.191730  0.000000\n",
       "v   0.076816  0.000000  0.0  0.028336  0.000000\n",
       "w   0.055639  0.000000  0.0  0.133406  0.031008\n",
       "x   0.292907  0.000000  0.0  0.180077  0.326472\n",
       "y   0.000000  0.450249  0.0  0.152695  0.032956\n",
       "z   0.000000  0.000000  0.0  0.017561  0.000000\n",
       "{   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "|   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "}   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "~   0.000000  0.000000  0.0  0.000000  0.000000\n",
       "\n",
       "[68 rows x 5 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]['document_matrix'].head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(os.path.join(DATA_PATH, 'pipeline_instance.pickle'),'wb') as datafile:\n",
    "#         pickle.dump(pipeline_instance, datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [128, 128]\n",
    "activations = ['relu']\n",
    "dropout = [0.15]\n",
    "attention = [128]\n",
    "max(len(layers), len(activations), len(dropout),  len(attention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 21082)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          2698624     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128)          512         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_probs (Dense)         (None, 128)          16512       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Multiply)        (None, 128)          0           batch_normalization_1[0][0]      \n",
      "                                                                 attention_probs[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 11)           1419        attention_mul[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 11)           0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,717,067\n",
      "Trainable params: 2,716,811\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = simple_ffn(document_matrix, labels)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10139, 68)\n",
      "Train on 9125 samples, validate on 1014 samples\n",
      "Epoch 1/100\n",
      "9125/9125 [==============================] - 1s 131us/step - loss: 2.3898 - acc: 0.2655 - val_loss: 2.3824 - val_acc: 0.3402\n",
      "Epoch 2/100\n",
      "9125/9125 [==============================] - 1s 89us/step - loss: 2.3738 - acc: 0.3468 - val_loss: 2.3614 - val_acc: 0.3205\n",
      "Epoch 3/100\n",
      "9125/9125 [==============================] - 1s 89us/step - loss: 2.3360 - acc: 0.3395 - val_loss: 2.3005 - val_acc: 0.3323\n",
      "Epoch 4/100\n",
      "9125/9125 [==============================] - 1s 68us/step - loss: 2.2720 - acc: 0.3530 - val_loss: 2.2307 - val_acc: 0.3728\n",
      "Epoch 5/100\n",
      "9125/9125 [==============================] - 1s 69us/step - loss: 2.2051 - acc: 0.3784 - val_loss: 2.1467 - val_acc: 0.3895\n",
      "Epoch 6/100\n",
      "9125/9125 [==============================] - 1s 69us/step - loss: 2.1287 - acc: 0.4169 - val_loss: 2.0746 - val_acc: 0.4783\n",
      "Epoch 7/100\n",
      "9125/9125 [==============================] - 1s 134us/step - loss: 2.0666 - acc: 0.4671 - val_loss: 2.0086 - val_acc: 0.5276\n",
      "Epoch 8/100\n",
      "9125/9125 [==============================] - 1s 69us/step - loss: 2.0053 - acc: 0.5022 - val_loss: 1.9493 - val_acc: 0.5690\n",
      "Epoch 9/100\n",
      "9125/9125 [==============================] - 1s 68us/step - loss: 1.9452 - acc: 0.5332 - val_loss: 1.8799 - val_acc: 0.6065\n",
      "Epoch 10/100\n",
      "9125/9125 [==============================] - 1s 69us/step - loss: 1.8805 - acc: 0.5638 - val_loss: 1.8100 - val_acc: 0.6243\n",
      "Epoch 11/100\n",
      "9125/9125 [==============================] - 1s 105us/step - loss: 1.8226 - acc: 0.5813 - val_loss: 1.7485 - val_acc: 0.6391\n",
      "Epoch 12/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 1.7756 - acc: 0.5913 - val_loss: 1.6939 - val_acc: 0.6479\n",
      "Epoch 13/100\n",
      "9125/9125 [==============================] - 1s 84us/step - loss: 1.7175 - acc: 0.6069 - val_loss: 1.6361 - val_acc: 0.6627\n",
      "Epoch 14/100\n",
      "9125/9125 [==============================] - 1s 109us/step - loss: 1.6603 - acc: 0.6111 - val_loss: 1.5787 - val_acc: 0.6637\n",
      "Epoch 15/100\n",
      "9125/9125 [==============================] - 1s 99us/step - loss: 1.6115 - acc: 0.6110 - val_loss: 1.5279 - val_acc: 0.6647\n",
      "Epoch 16/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 1.5730 - acc: 0.6103 - val_loss: 1.4792 - val_acc: 0.6657\n",
      "Epoch 17/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 1.5269 - acc: 0.6148 - val_loss: 1.4361 - val_acc: 0.6696\n",
      "Epoch 18/100\n",
      "9125/9125 [==============================] - 1s 119us/step - loss: 1.4932 - acc: 0.6127 - val_loss: 1.3946 - val_acc: 0.6726\n",
      "Epoch 19/100\n",
      "9125/9125 [==============================] - 1s 85us/step - loss: 1.4556 - acc: 0.6162 - val_loss: 1.3564 - val_acc: 0.6765\n",
      "Epoch 20/100\n",
      "9125/9125 [==============================] - 1s 80us/step - loss: 1.4187 - acc: 0.6179 - val_loss: 1.3219 - val_acc: 0.6785\n",
      "Epoch 21/100\n",
      "9125/9125 [==============================] - 1s 88us/step - loss: 1.3871 - acc: 0.6170 - val_loss: 1.2810 - val_acc: 0.6805\n",
      "Epoch 22/100\n",
      "9125/9125 [==============================] - 1s 110us/step - loss: 1.3657 - acc: 0.6139 - val_loss: 1.2451 - val_acc: 0.6805\n",
      "Epoch 23/100\n",
      "9125/9125 [==============================] - 1s 75us/step - loss: 1.3245 - acc: 0.6229 - val_loss: 1.2145 - val_acc: 0.6775\n",
      "Epoch 24/100\n",
      "9125/9125 [==============================] - 1s 79us/step - loss: 1.2911 - acc: 0.6233 - val_loss: 1.1870 - val_acc: 0.6726\n",
      "Epoch 25/100\n",
      "9125/9125 [==============================] - 1s 120us/step - loss: 1.2715 - acc: 0.6209 - val_loss: 1.1568 - val_acc: 0.6755\n",
      "Epoch 26/100\n",
      "9125/9125 [==============================] - 1s 84us/step - loss: 1.2447 - acc: 0.6243 - val_loss: 1.1332 - val_acc: 0.6824\n",
      "Epoch 27/100\n",
      "9125/9125 [==============================] - 1s 109us/step - loss: 1.2231 - acc: 0.6256 - val_loss: 1.1045 - val_acc: 0.6805\n",
      "Epoch 28/100\n",
      "9125/9125 [==============================] - 3s 314us/step - loss: 1.1932 - acc: 0.6325 - val_loss: 1.0787 - val_acc: 0.6824\n",
      "Epoch 29/100\n",
      "9125/9125 [==============================] - 1s 84us/step - loss: 1.1789 - acc: 0.6316 - val_loss: 1.0596 - val_acc: 0.6834\n",
      "Epoch 30/100\n",
      "9125/9125 [==============================] - 1s 105us/step - loss: 1.1616 - acc: 0.6279 - val_loss: 1.0339 - val_acc: 0.6834\n",
      "Epoch 31/100\n",
      "9125/9125 [==============================] - 1s 78us/step - loss: 1.1362 - acc: 0.6341 - val_loss: 1.0086 - val_acc: 0.6834\n",
      "Epoch 32/100\n",
      "9125/9125 [==============================] - 1s 69us/step - loss: 1.1244 - acc: 0.6312 - val_loss: 0.9930 - val_acc: 0.6844\n",
      "Epoch 33/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 1.1125 - acc: 0.6301 - val_loss: 0.9722 - val_acc: 0.6844\n",
      "Epoch 34/100\n",
      "9125/9125 [==============================] - 1s 116us/step - loss: 1.0866 - acc: 0.6367 - val_loss: 0.9552 - val_acc: 0.6884\n",
      "Epoch 35/100\n",
      "9125/9125 [==============================] - 1s 79us/step - loss: 1.0779 - acc: 0.6328 - val_loss: 0.9437 - val_acc: 0.6864\n",
      "Epoch 36/100\n",
      "9125/9125 [==============================] - 1s 68us/step - loss: 1.0609 - acc: 0.6364 - val_loss: 0.9259 - val_acc: 0.6874\n",
      "Epoch 37/100\n",
      "9125/9125 [==============================] - 1s 75us/step - loss: 1.0555 - acc: 0.6298 - val_loss: 0.9127 - val_acc: 0.6874\n",
      "Epoch 38/100\n",
      "9125/9125 [==============================] - 1s 109us/step - loss: 1.0260 - acc: 0.6422 - val_loss: 0.9001 - val_acc: 0.6903\n",
      "Epoch 39/100\n",
      "9125/9125 [==============================] - 1s 76us/step - loss: 1.0161 - acc: 0.6421 - val_loss: 0.8836 - val_acc: 0.6923\n",
      "Epoch 40/100\n",
      "9125/9125 [==============================] - 1s 69us/step - loss: 1.0094 - acc: 0.6404 - val_loss: 0.8726 - val_acc: 0.6953\n",
      "Epoch 41/100\n",
      "9125/9125 [==============================] - 1s 74us/step - loss: 0.9912 - acc: 0.6464 - val_loss: 0.8624 - val_acc: 0.6972\n",
      "Epoch 42/100\n",
      "9125/9125 [==============================] - 1s 114us/step - loss: 0.9905 - acc: 0.6431 - val_loss: 0.8489 - val_acc: 0.6982\n",
      "Epoch 43/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 0.9737 - acc: 0.6448 - val_loss: 0.8402 - val_acc: 0.7012\n",
      "Epoch 44/100\n",
      "9125/9125 [==============================] - 1s 71us/step - loss: 0.9611 - acc: 0.6506 - val_loss: 0.8314 - val_acc: 0.7041\n",
      "Epoch 45/100\n",
      "9125/9125 [==============================] - 1s 75us/step - loss: 0.9655 - acc: 0.6493 - val_loss: 0.8206 - val_acc: 0.7051\n",
      "Epoch 46/100\n",
      "9125/9125 [==============================] - 1s 107us/step - loss: 0.9520 - acc: 0.6571 - val_loss: 0.8105 - val_acc: 0.7110\n",
      "Epoch 47/100\n",
      "9125/9125 [==============================] - 1s 75us/step - loss: 0.9368 - acc: 0.6663 - val_loss: 0.8005 - val_acc: 0.7209\n",
      "Epoch 48/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 0.9198 - acc: 0.6713 - val_loss: 0.7893 - val_acc: 0.7268\n",
      "Epoch 49/100\n",
      "9125/9125 [==============================] - 1s 74us/step - loss: 0.9099 - acc: 0.6745 - val_loss: 0.7759 - val_acc: 0.7367\n",
      "Epoch 50/100\n",
      "9125/9125 [==============================] - 1s 119us/step - loss: 0.8989 - acc: 0.6855 - val_loss: 0.7650 - val_acc: 0.7377\n",
      "Epoch 51/100\n",
      "9125/9125 [==============================] - 1s 76us/step - loss: 0.8890 - acc: 0.6843 - val_loss: 0.7579 - val_acc: 0.7465\n",
      "Epoch 52/100\n",
      "9125/9125 [==============================] - 1s 71us/step - loss: 0.8703 - acc: 0.6941 - val_loss: 0.7463 - val_acc: 0.7475\n",
      "Epoch 53/100\n",
      "9125/9125 [==============================] - 1s 100us/step - loss: 0.8703 - acc: 0.6887 - val_loss: 0.7396 - val_acc: 0.7525\n",
      "Epoch 54/100\n",
      "9125/9125 [==============================] - 1s 97us/step - loss: 0.8594 - acc: 0.6916 - val_loss: 0.7280 - val_acc: 0.7515\n",
      "Epoch 55/100\n",
      "9125/9125 [==============================] - 1s 117us/step - loss: 0.8590 - acc: 0.6873 - val_loss: 0.7218 - val_acc: 0.7475\n",
      "Epoch 56/100\n",
      "9125/9125 [==============================] - 2s 201us/step - loss: 0.8424 - acc: 0.6963 - val_loss: 0.7151 - val_acc: 0.7505\n",
      "Epoch 57/100\n",
      "9125/9125 [==============================] - 1s 79us/step - loss: 0.8370 - acc: 0.6958 - val_loss: 0.7059 - val_acc: 0.7475\n",
      "Epoch 58/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 0.8307 - acc: 0.6996 - val_loss: 0.7008 - val_acc: 0.7465\n",
      "Epoch 59/100\n",
      "9125/9125 [==============================] - 1s 115us/step - loss: 0.8184 - acc: 0.7003 - val_loss: 0.6947 - val_acc: 0.7475\n",
      "Epoch 60/100\n",
      "9125/9125 [==============================] - 1s 79us/step - loss: 0.8168 - acc: 0.6987 - val_loss: 0.6873 - val_acc: 0.7525\n",
      "Epoch 61/100\n",
      "9125/9125 [==============================] - 1s 70us/step - loss: 0.8053 - acc: 0.7021 - val_loss: 0.6860 - val_acc: 0.7495\n",
      "Epoch 62/100\n",
      "9125/9125 [==============================] - 1s 68us/step - loss: 0.7953 - acc: 0.7052 - val_loss: 0.6803 - val_acc: 0.7485\n",
      "Epoch 63/100\n",
      "9125/9125 [==============================] - 1s 100us/step - loss: 0.7928 - acc: 0.7078 - val_loss: 0.6722 - val_acc: 0.7505\n",
      "Epoch 64/100\n",
      "9125/9125 [==============================] - 1s 82us/step - loss: 0.7871 - acc: 0.7054 - val_loss: 0.6704 - val_acc: 0.7515\n",
      "Epoch 65/100\n",
      "9125/9125 [==============================] - 1s 70us/step - loss: 0.7912 - acc: 0.7053 - val_loss: 0.6638 - val_acc: 0.7505\n",
      "Epoch 66/100\n",
      "9125/9125 [==============================] - 1s 65us/step - loss: 0.7735 - acc: 0.7099 - val_loss: 0.6582 - val_acc: 0.7525\n",
      "Epoch 67/100\n",
      "9125/9125 [==============================] - 1s 89us/step - loss: 0.7655 - acc: 0.7123 - val_loss: 0.6548 - val_acc: 0.7544\n",
      "Epoch 68/100\n",
      "9125/9125 [==============================] - 1s 93us/step - loss: 0.7625 - acc: 0.7117 - val_loss: 0.6505 - val_acc: 0.7535\n",
      "Epoch 69/100\n",
      "9125/9125 [==============================] - 1s 71us/step - loss: 0.7533 - acc: 0.7139 - val_loss: 0.6463 - val_acc: 0.7564\n",
      "Epoch 70/100\n",
      "9125/9125 [==============================] - 1s 68us/step - loss: 0.7397 - acc: 0.7196 - val_loss: 0.6438 - val_acc: 0.7535\n",
      "Epoch 71/100\n",
      "9125/9125 [==============================] - 1s 79us/step - loss: 0.7491 - acc: 0.7151 - val_loss: 0.6395 - val_acc: 0.7564\n",
      "Epoch 72/100\n",
      "9125/9125 [==============================] - 1s 113us/step - loss: 0.7417 - acc: 0.7182 - val_loss: 0.6343 - val_acc: 0.7564\n",
      "Epoch 73/100\n",
      "9125/9125 [==============================] - 1s 71us/step - loss: 0.7374 - acc: 0.7204 - val_loss: 0.6311 - val_acc: 0.7574\n",
      "Epoch 74/100\n",
      "9125/9125 [==============================] - 1s 75us/step - loss: 0.7273 - acc: 0.7220 - val_loss: 0.6271 - val_acc: 0.7574\n",
      "Epoch 75/100\n",
      "9125/9125 [==============================] - 1s 103us/step - loss: 0.7162 - acc: 0.7253 - val_loss: 0.6259 - val_acc: 0.7604\n",
      "Epoch 76/100\n",
      "9125/9125 [==============================] - 1s 83us/step - loss: 0.7215 - acc: 0.7257 - val_loss: 0.6226 - val_acc: 0.7613\n",
      "Epoch 77/100\n",
      "9125/9125 [==============================] - 1s 73us/step - loss: 0.7220 - acc: 0.7250 - val_loss: 0.6175 - val_acc: 0.7663\n",
      "Epoch 78/100\n",
      "9125/9125 [==============================] - 1s 67us/step - loss: 0.7076 - acc: 0.7363 - val_loss: 0.6150 - val_acc: 0.7712\n",
      "Epoch 79/100\n",
      "9125/9125 [==============================] - 1s 87us/step - loss: 0.7113 - acc: 0.7367 - val_loss: 0.6125 - val_acc: 0.7722\n",
      "Epoch 80/100\n",
      "9125/9125 [==============================] - 1s 103us/step - loss: 0.7090 - acc: 0.7413 - val_loss: 0.6084 - val_acc: 0.7781\n",
      "Epoch 81/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 0.7013 - acc: 0.7402 - val_loss: 0.6054 - val_acc: 0.7801\n",
      "Epoch 82/100\n",
      "9125/9125 [==============================] - 1s 67us/step - loss: 0.6959 - acc: 0.7458 - val_loss: 0.6039 - val_acc: 0.7850\n",
      "Epoch 83/100\n",
      "9125/9125 [==============================] - 1s 104us/step - loss: 0.6904 - acc: 0.7424 - val_loss: 0.6010 - val_acc: 0.7860\n",
      "Epoch 84/100\n",
      "9125/9125 [==============================] - 1s 81us/step - loss: 0.6908 - acc: 0.7510 - val_loss: 0.5982 - val_acc: 0.7870\n",
      "Epoch 85/100\n",
      "9125/9125 [==============================] - 1s 78us/step - loss: 0.6882 - acc: 0.7495 - val_loss: 0.5957 - val_acc: 0.7860\n",
      "Epoch 86/100\n",
      "9125/9125 [==============================] - 1s 69us/step - loss: 0.6752 - acc: 0.7546 - val_loss: 0.5945 - val_acc: 0.7860\n",
      "Epoch 87/100\n",
      "9125/9125 [==============================] - 1s 104us/step - loss: 0.6770 - acc: 0.7523 - val_loss: 0.5916 - val_acc: 0.7860\n",
      "Epoch 88/100\n",
      "9125/9125 [==============================] - 1s 80us/step - loss: 0.6854 - acc: 0.7525 - val_loss: 0.5889 - val_acc: 0.7860\n",
      "Epoch 89/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 0.6702 - acc: 0.7532 - val_loss: 0.5868 - val_acc: 0.7880\n",
      "Epoch 90/100\n",
      "9125/9125 [==============================] - 1s 68us/step - loss: 0.6590 - acc: 0.7611 - val_loss: 0.5848 - val_acc: 0.7870\n",
      "Epoch 91/100\n",
      "9125/9125 [==============================] - 1s 89us/step - loss: 0.6701 - acc: 0.7557 - val_loss: 0.5826 - val_acc: 0.7880\n",
      "Epoch 92/100\n",
      "9125/9125 [==============================] - 1s 96us/step - loss: 0.6679 - acc: 0.7569 - val_loss: 0.5798 - val_acc: 0.7899\n",
      "Epoch 93/100\n",
      "9125/9125 [==============================] - 1s 72us/step - loss: 0.6677 - acc: 0.7581 - val_loss: 0.5778 - val_acc: 0.7870\n",
      "Epoch 94/100\n",
      "9125/9125 [==============================] - 1s 66us/step - loss: 0.6595 - acc: 0.7582 - val_loss: 0.5762 - val_acc: 0.7919\n",
      "Epoch 95/100\n",
      "9125/9125 [==============================] - 1s 88us/step - loss: 0.6568 - acc: 0.7613 - val_loss: 0.5753 - val_acc: 0.7899\n",
      "Epoch 96/100\n",
      "9125/9125 [==============================] - 1s 100us/step - loss: 0.6606 - acc: 0.7575 - val_loss: 0.5738 - val_acc: 0.7909\n",
      "Epoch 97/100\n",
      "9125/9125 [==============================] - 1s 71us/step - loss: 0.6449 - acc: 0.7623 - val_loss: 0.5717 - val_acc: 0.7899\n",
      "Epoch 98/100\n",
      "9125/9125 [==============================] - 1s 69us/step - loss: 0.6450 - acc: 0.7642 - val_loss: 0.5707 - val_acc: 0.7919\n",
      "Epoch 99/100\n",
      "9125/9125 [==============================] - 1s 87us/step - loss: 0.6494 - acc: 0.7642 - val_loss: 0.5698 - val_acc: 0.7899\n",
      "Epoch 100/100\n",
      "9125/9125 [==============================] - 1s 101us/step - loss: 0.6505 - acc: 0.7611 - val_loss: 0.5684 - val_acc: 0.7899\n",
      "(10139, 2161)\n",
      "Train on 9125 samples, validate on 1014 samples\n",
      "Epoch 1/100\n",
      "9125/9125 [==============================] - 5s 523us/step - loss: 2.3764 - acc: 0.4974 - val_loss: 2.3286 - val_acc: 0.7022\n",
      "Epoch 2/100\n",
      "9125/9125 [==============================] - 4s 427us/step - loss: 2.2714 - acc: 0.6612 - val_loss: 2.1513 - val_acc: 0.7613\n",
      "Epoch 3/100\n",
      "9125/9125 [==============================] - 4s 420us/step - loss: 2.1422 - acc: 0.6308 - val_loss: 2.0247 - val_acc: 0.7347\n",
      "Epoch 4/100\n",
      "9125/9125 [==============================] - 4s 455us/step - loss: 2.0223 - acc: 0.6023 - val_loss: 1.8930 - val_acc: 0.6953\n",
      "Epoch 5/100\n",
      "9125/9125 [==============================] - 4s 446us/step - loss: 1.9274 - acc: 0.5995 - val_loss: 1.8089 - val_acc: 0.7012\n",
      "Epoch 6/100\n",
      "9125/9125 [==============================] - 4s 460us/step - loss: 1.8542 - acc: 0.6079 - val_loss: 1.7436 - val_acc: 0.7140\n",
      "Epoch 7/100\n",
      "9125/9125 [==============================] - 4s 474us/step - loss: 1.7980 - acc: 0.6135 - val_loss: 1.6791 - val_acc: 0.7308\n",
      "Epoch 8/100\n",
      "9125/9125 [==============================] - 4s 420us/step - loss: 1.7366 - acc: 0.6026 - val_loss: 1.5974 - val_acc: 0.7032\n",
      "Epoch 9/100\n",
      "9125/9125 [==============================] - 4s 413us/step - loss: 1.6549 - acc: 0.6212 - val_loss: 1.5315 - val_acc: 0.7298\n",
      "Epoch 10/100\n",
      "9125/9125 [==============================] - 4s 459us/step - loss: 1.5953 - acc: 0.6264 - val_loss: 1.4644 - val_acc: 0.7278\n",
      "Epoch 11/100\n",
      "9125/9125 [==============================] - 4s 430us/step - loss: 1.5346 - acc: 0.6329 - val_loss: 1.4109 - val_acc: 0.7288\n",
      "Epoch 12/100\n",
      "9125/9125 [==============================] - 4s 426us/step - loss: 1.4800 - acc: 0.6340 - val_loss: 1.3472 - val_acc: 0.7337\n",
      "Epoch 13/100\n",
      "9125/9125 [==============================] - 4s 458us/step - loss: 1.4303 - acc: 0.6358 - val_loss: 1.2983 - val_acc: 0.7347\n",
      "Epoch 14/100\n",
      "9125/9125 [==============================] - 4s 422us/step - loss: 1.3910 - acc: 0.6402 - val_loss: 1.2512 - val_acc: 0.7337\n",
      "Epoch 15/100\n",
      "9125/9125 [==============================] - 7s 723us/step - loss: 1.3638 - acc: 0.6292 - val_loss: 1.1977 - val_acc: 0.7367\n",
      "Epoch 16/100\n",
      "9125/9125 [==============================] - 4s 481us/step - loss: 1.3171 - acc: 0.6346 - val_loss: 1.1640 - val_acc: 0.7387\n",
      "Epoch 17/100\n",
      "9125/9125 [==============================] - 4s 437us/step - loss: 1.2730 - acc: 0.6421 - val_loss: 1.1246 - val_acc: 0.7416\n",
      "Epoch 18/100\n",
      "9125/9125 [==============================] - 4s 424us/step - loss: 1.2440 - acc: 0.6388 - val_loss: 1.0849 - val_acc: 0.7406\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9125/9125 [==============================] - 4s 432us/step - loss: 1.2055 - acc: 0.6450 - val_loss: 1.0463 - val_acc: 0.7426\n",
      "Epoch 20/100\n",
      "9125/9125 [==============================] - 4s 420us/step - loss: 1.1855 - acc: 0.6412 - val_loss: 1.0136 - val_acc: 0.7406\n",
      "Epoch 21/100\n",
      "9125/9125 [==============================] - 4s 449us/step - loss: 1.1567 - acc: 0.6401 - val_loss: 0.9818 - val_acc: 0.7396\n",
      "Epoch 22/100\n",
      "9125/9125 [==============================] - 4s 436us/step - loss: 1.1214 - acc: 0.6462 - val_loss: 0.9541 - val_acc: 0.7396\n",
      "Epoch 23/100\n",
      "9125/9125 [==============================] - 4s 483us/step - loss: 1.0945 - acc: 0.6503 - val_loss: 0.9288 - val_acc: 0.7377\n",
      "Epoch 24/100\n",
      "9125/9125 [==============================] - 4s 399us/step - loss: 1.0878 - acc: 0.6376 - val_loss: 0.9026 - val_acc: 0.7367\n",
      "Epoch 25/100\n",
      "9125/9125 [==============================] - 4s 432us/step - loss: 1.0638 - acc: 0.6412 - val_loss: 0.8742 - val_acc: 0.7377\n",
      "Epoch 26/100\n",
      "9125/9125 [==============================] - 4s 424us/step - loss: 1.0389 - acc: 0.6460 - val_loss: 0.8469 - val_acc: 0.7357\n",
      "Epoch 27/100\n",
      "9125/9125 [==============================] - 4s 409us/step - loss: 1.0192 - acc: 0.6504 - val_loss: 0.8291 - val_acc: 0.7357\n",
      "Epoch 28/100\n",
      "9125/9125 [==============================] - 4s 403us/step - loss: 1.0001 - acc: 0.6529 - val_loss: 0.8063 - val_acc: 0.7367\n",
      "Epoch 29/100\n",
      "9125/9125 [==============================] - 4s 442us/step - loss: 0.9781 - acc: 0.6530 - val_loss: 0.7910 - val_acc: 0.7396\n",
      "Epoch 30/100\n",
      "9125/9125 [==============================] - 4s 414us/step - loss: 0.9667 - acc: 0.6528 - val_loss: 0.7675 - val_acc: 0.7387\n",
      "Epoch 31/100\n",
      "9125/9125 [==============================] - 4s 428us/step - loss: 0.9632 - acc: 0.6456 - val_loss: 0.7510 - val_acc: 0.7387\n",
      "Epoch 32/100\n",
      "9125/9125 [==============================] - 4s 435us/step - loss: 0.9405 - acc: 0.6523 - val_loss: 0.7411 - val_acc: 0.7485\n",
      "Epoch 33/100\n",
      "9125/9125 [==============================] - 4s 445us/step - loss: 0.9191 - acc: 0.6643 - val_loss: 0.7205 - val_acc: 0.7485\n",
      "Epoch 34/100\n",
      "9125/9125 [==============================] - 4s 409us/step - loss: 0.9040 - acc: 0.6654 - val_loss: 0.7099 - val_acc: 0.7495\n",
      "Epoch 35/100\n",
      "9125/9125 [==============================] - 4s 396us/step - loss: 0.8931 - acc: 0.6665 - val_loss: 0.6974 - val_acc: 0.7515\n",
      "Epoch 36/100\n",
      "9125/9125 [==============================] - 5s 509us/step - loss: 0.8972 - acc: 0.6586 - val_loss: 0.6812 - val_acc: 0.7505\n",
      "Epoch 37/100\n",
      "9125/9125 [==============================] - 4s 403us/step - loss: 0.8729 - acc: 0.6655 - val_loss: 0.6702 - val_acc: 0.7525\n",
      "Epoch 38/100\n",
      "9125/9125 [==============================] - 4s 412us/step - loss: 0.8561 - acc: 0.6716 - val_loss: 0.6594 - val_acc: 0.7535\n",
      "Epoch 39/100\n",
      "9125/9125 [==============================] - 4s 449us/step - loss: 0.8564 - acc: 0.6681 - val_loss: 0.6506 - val_acc: 0.7564\n",
      "Epoch 40/100\n",
      "9125/9125 [==============================] - 4s 420us/step - loss: 0.8563 - acc: 0.6637 - val_loss: 0.6391 - val_acc: 0.7564\n",
      "Epoch 41/100\n",
      "9125/9125 [==============================] - 4s 409us/step - loss: 0.8292 - acc: 0.6719 - val_loss: 0.6319 - val_acc: 0.7584\n",
      "Epoch 42/100\n",
      "9125/9125 [==============================] - 4s 449us/step - loss: 0.8174 - acc: 0.6734 - val_loss: 0.6256 - val_acc: 0.7564\n",
      "Epoch 43/100\n",
      "9125/9125 [==============================] - 4s 436us/step - loss: 0.8225 - acc: 0.6721 - val_loss: 0.6144 - val_acc: 0.7564\n",
      "Epoch 44/100\n",
      "9125/9125 [==============================] - 4s 408us/step - loss: 0.8175 - acc: 0.6752 - val_loss: 0.6091 - val_acc: 0.7574\n",
      "Epoch 45/100\n",
      "9125/9125 [==============================] - 6s 685us/step - loss: 0.7988 - acc: 0.6781 - val_loss: 0.6021 - val_acc: 0.7564\n",
      "Epoch 46/100\n",
      "9125/9125 [==============================] - 5s 494us/step - loss: 0.7954 - acc: 0.6804 - val_loss: 0.5975 - val_acc: 0.7564\n",
      "Epoch 47/100\n",
      "9125/9125 [==============================] - 4s 443us/step - loss: 0.7821 - acc: 0.6849 - val_loss: 0.5901 - val_acc: 0.7574\n",
      "Epoch 48/100\n",
      "9125/9125 [==============================] - 4s 417us/step - loss: 0.7834 - acc: 0.6781 - val_loss: 0.5857 - val_acc: 0.7584\n",
      "Epoch 49/100\n",
      "9125/9125 [==============================] - 4s 488us/step - loss: 0.7800 - acc: 0.6808 - val_loss: 0.5806 - val_acc: 0.7604\n",
      "Epoch 50/100\n",
      "9125/9125 [==============================] - 4s 455us/step - loss: 0.7631 - acc: 0.6868 - val_loss: 0.5763 - val_acc: 0.7574\n",
      "Epoch 51/100\n",
      "9125/9125 [==============================] - 4s 466us/step - loss: 0.7627 - acc: 0.6841 - val_loss: 0.5724 - val_acc: 0.7584\n",
      "Epoch 52/100\n",
      "9125/9125 [==============================] - 4s 440us/step - loss: 0.7669 - acc: 0.6818 - val_loss: 0.5677 - val_acc: 0.7604\n",
      "Epoch 53/100\n",
      "9125/9125 [==============================] - 4s 415us/step - loss: 0.7571 - acc: 0.6876 - val_loss: 0.5634 - val_acc: 0.7574\n",
      "Epoch 54/100\n",
      "9125/9125 [==============================] - 4s 456us/step - loss: 0.7458 - acc: 0.6887 - val_loss: 0.5607 - val_acc: 0.7594\n",
      "Epoch 55/100\n",
      "9125/9125 [==============================] - 4s 423us/step - loss: 0.7547 - acc: 0.6803 - val_loss: 0.5555 - val_acc: 0.7594\n",
      "Epoch 56/100\n",
      "9125/9125 [==============================] - 4s 411us/step - loss: 0.7423 - acc: 0.6896 - val_loss: 0.5542 - val_acc: 0.7584\n",
      "Epoch 57/100\n",
      "9125/9125 [==============================] - 4s 474us/step - loss: 0.7266 - acc: 0.6921 - val_loss: 0.5516 - val_acc: 0.7574\n",
      "Epoch 58/100\n",
      "9125/9125 [==============================] - 4s 450us/step - loss: 0.7195 - acc: 0.6948 - val_loss: 0.5508 - val_acc: 0.7574\n",
      "Epoch 59/100\n",
      "9125/9125 [==============================] - 4s 419us/step - loss: 0.7355 - acc: 0.6870 - val_loss: 0.5463 - val_acc: 0.7594\n",
      "Epoch 60/100\n",
      "9125/9125 [==============================] - 4s 466us/step - loss: 0.7275 - acc: 0.6908 - val_loss: 0.5434 - val_acc: 0.7673\n",
      "Epoch 61/100\n",
      "9125/9125 [==============================] - 4s 451us/step - loss: 0.7168 - acc: 0.6921 - val_loss: 0.5410 - val_acc: 0.7682\n",
      "Epoch 62/100\n",
      "9125/9125 [==============================] - 4s 419us/step - loss: 0.7208 - acc: 0.6934 - val_loss: 0.5405 - val_acc: 0.7702\n",
      "Epoch 63/100\n",
      "9125/9125 [==============================] - 4s 481us/step - loss: 0.7024 - acc: 0.6963 - val_loss: 0.5374 - val_acc: 0.7692\n",
      "Epoch 64/100\n",
      "9125/9125 [==============================] - 4s 423us/step - loss: 0.7011 - acc: 0.6984 - val_loss: 0.5356 - val_acc: 0.7692\n",
      "Epoch 65/100\n",
      "9125/9125 [==============================] - 4s 480us/step - loss: 0.7001 - acc: 0.7016 - val_loss: 0.5340 - val_acc: 0.7692\n",
      "Epoch 66/100\n",
      "9125/9125 [==============================] - 4s 488us/step - loss: 0.7021 - acc: 0.6985 - val_loss: 0.5345 - val_acc: 0.7682\n",
      "Epoch 67/100\n",
      "9125/9125 [==============================] - 5s 577us/step - loss: 0.6907 - acc: 0.7027 - val_loss: 0.5337 - val_acc: 0.7682\n",
      "Epoch 68/100\n",
      "9125/9125 [==============================] - 4s 424us/step - loss: 0.6861 - acc: 0.7064 - val_loss: 0.5338 - val_acc: 0.7682\n",
      "Epoch 69/100\n",
      "9125/9125 [==============================] - 4s 417us/step - loss: 0.6871 - acc: 0.7041 - val_loss: 0.5317 - val_acc: 0.7692\n",
      "Epoch 70/100\n",
      "9125/9125 [==============================] - 4s 458us/step - loss: 0.6797 - acc: 0.7054 - val_loss: 0.5295 - val_acc: 0.7692\n",
      "Epoch 71/100\n",
      "9125/9125 [==============================] - 4s 439us/step - loss: 0.6712 - acc: 0.7089 - val_loss: 0.5289 - val_acc: 0.7692\n",
      "Epoch 72/100\n",
      "9125/9125 [==============================] - 4s 414us/step - loss: 0.6706 - acc: 0.7085 - val_loss: 0.5291 - val_acc: 0.7692\n",
      "Epoch 73/100\n",
      "9125/9125 [==============================] - 4s 478us/step - loss: 0.6645 - acc: 0.7122 - val_loss: 0.5300 - val_acc: 0.7692\n",
      "Epoch 74/100\n",
      "9125/9125 [==============================] - 7s 749us/step - loss: 0.6576 - acc: 0.7135 - val_loss: 0.5278 - val_acc: 0.7692\n",
      "Epoch 75/100\n",
      "9125/9125 [==============================] - 4s 431us/step - loss: 0.6529 - acc: 0.7168 - val_loss: 0.5270 - val_acc: 0.7692\n",
      "Epoch 76/100\n",
      "9125/9125 [==============================] - 4s 474us/step - loss: 0.6483 - acc: 0.7197 - val_loss: 0.5264 - val_acc: 0.7692\n",
      "Epoch 77/100\n",
      "9125/9125 [==============================] - 4s 424us/step - loss: 0.6589 - acc: 0.7110 - val_loss: 0.5248 - val_acc: 0.7692\n",
      "Epoch 78/100\n",
      "9125/9125 [==============================] - 4s 406us/step - loss: 0.6470 - acc: 0.7180 - val_loss: 0.5251 - val_acc: 0.7702\n",
      "Epoch 79/100\n",
      "9125/9125 [==============================] - 4s 486us/step - loss: 0.6398 - acc: 0.7222 - val_loss: 0.5255 - val_acc: 0.7692\n",
      "Epoch 80/100\n",
      "9125/9125 [==============================] - 4s 414us/step - loss: 0.6356 - acc: 0.7268 - val_loss: 0.5253 - val_acc: 0.7702\n",
      "Epoch 81/100\n",
      "9125/9125 [==============================] - 4s 401us/step - loss: 0.6325 - acc: 0.7299 - val_loss: 0.5231 - val_acc: 0.7712\n",
      "Epoch 82/100\n",
      "9125/9125 [==============================] - 4s 440us/step - loss: 0.6392 - acc: 0.7292 - val_loss: 0.5223 - val_acc: 0.7712\n",
      "Epoch 83/100\n",
      "9125/9125 [==============================] - 4s 413us/step - loss: 0.6196 - acc: 0.7339 - val_loss: 0.5208 - val_acc: 0.7712\n",
      "Epoch 84/100\n",
      "9125/9125 [==============================] - 4s 416us/step - loss: 0.6192 - acc: 0.7369 - val_loss: 0.5225 - val_acc: 0.7712\n",
      "Epoch 85/100\n",
      "9125/9125 [==============================] - 4s 410us/step - loss: 0.6114 - acc: 0.7397 - val_loss: 0.5215 - val_acc: 0.7742\n",
      "Epoch 86/100\n",
      "9125/9125 [==============================] - 4s 448us/step - loss: 0.6097 - acc: 0.7379 - val_loss: 0.5199 - val_acc: 0.7742\n",
      "Epoch 87/100\n",
      "9125/9125 [==============================] - 4s 415us/step - loss: 0.6065 - acc: 0.7425 - val_loss: 0.5175 - val_acc: 0.7742\n",
      "Epoch 88/100\n",
      "9125/9125 [==============================] - 4s 413us/step - loss: 0.5968 - acc: 0.7419 - val_loss: 0.5186 - val_acc: 0.7761\n",
      "Epoch 89/100\n",
      "9125/9125 [==============================] - 4s 438us/step - loss: 0.6053 - acc: 0.7440 - val_loss: 0.5187 - val_acc: 0.7761\n",
      "Epoch 90/100\n",
      "9125/9125 [==============================] - 4s 428us/step - loss: 0.5983 - acc: 0.7462 - val_loss: 0.5192 - val_acc: 0.7751\n",
      "Epoch 91/100\n",
      "9125/9125 [==============================] - 4s 410us/step - loss: 0.5829 - acc: 0.7512 - val_loss: 0.5185 - val_acc: 0.7742\n",
      "Epoch 92/100\n",
      "9125/9125 [==============================] - 4s 413us/step - loss: 0.5877 - acc: 0.7532 - val_loss: 0.5201 - val_acc: 0.7751\n",
      "Epoch 93/100\n",
      "9125/9125 [==============================] - 4s 446us/step - loss: 0.5832 - acc: 0.7521 - val_loss: 0.5176 - val_acc: 0.7751\n",
      "Epoch 94/100\n",
      "9125/9125 [==============================] - 4s 463us/step - loss: 0.5750 - acc: 0.7525 - val_loss: 0.5198 - val_acc: 0.7771\n",
      "Epoch 95/100\n",
      "9125/9125 [==============================] - 4s 431us/step - loss: 0.5788 - acc: 0.7535 - val_loss: 0.5239 - val_acc: 0.7781\n",
      "Epoch 96/100\n",
      "9125/9125 [==============================] - 5s 546us/step - loss: 0.5738 - acc: 0.7558 - val_loss: 0.5221 - val_acc: 0.7771\n",
      "Epoch 97/100\n",
      "9125/9125 [==============================] - 8s 860us/step - loss: 0.5615 - acc: 0.7588 - val_loss: 0.5207 - val_acc: 0.7791\n",
      "Epoch 98/100\n",
      "9125/9125 [==============================] - 7s 747us/step - loss: 0.5689 - acc: 0.7667 - val_loss: 0.5223 - val_acc: 0.7761\n",
      "Epoch 99/100\n",
      "9125/9125 [==============================] - 4s 452us/step - loss: 0.5630 - acc: 0.7641 - val_loss: 0.5227 - val_acc: 0.7761\n",
      "Epoch 100/100\n",
      "9125/9125 [==============================] - 4s 448us/step - loss: 0.5544 - acc: 0.7707 - val_loss: 0.5215 - val_acc: 0.7751\n",
      "(10139, 21082)\n",
      "Train on 9125 samples, validate on 1014 samples\n",
      "Epoch 1/100\n",
      "9125/9125 [==============================] - 43s 5ms/step - loss: 2.3430 - acc: 0.6861 - val_loss: 2.1832 - val_acc: 0.7110\n",
      "Epoch 2/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 2.1707 - acc: 0.5938 - val_loss: 2.0433 - val_acc: 0.6519\n",
      "Epoch 3/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 2.0469 - acc: 0.5855 - val_loss: 1.9369 - val_acc: 0.6864\n",
      "Epoch 4/100\n",
      "9125/9125 [==============================] - 41s 5ms/step - loss: 1.9589 - acc: 0.5907 - val_loss: 1.8619 - val_acc: 0.6874\n",
      "Epoch 5/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 1.8918 - acc: 0.5934 - val_loss: 1.8047 - val_acc: 0.6884\n",
      "Epoch 6/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 1.8324 - acc: 0.5979 - val_loss: 1.7516 - val_acc: 0.6884\n",
      "Epoch 7/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 1.7811 - acc: 0.5965 - val_loss: 1.6924 - val_acc: 0.6884\n",
      "Epoch 8/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 1.7258 - acc: 0.5978 - val_loss: 1.6338 - val_acc: 0.6874\n",
      "Epoch 9/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 1.6719 - acc: 0.5976 - val_loss: 1.5817 - val_acc: 0.6864\n",
      "Epoch 10/100\n",
      "9125/9125 [==============================] - 41s 5ms/step - loss: 1.6230 - acc: 0.6020 - val_loss: 1.5394 - val_acc: 0.6864\n",
      "Epoch 11/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 1.5801 - acc: 0.6027 - val_loss: 1.4852 - val_acc: 0.6874\n",
      "Epoch 12/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 1.5324 - acc: 0.6072 - val_loss: 1.4500 - val_acc: 0.6874\n",
      "Epoch 13/100\n",
      "9125/9125 [==============================] - 41s 4ms/step - loss: 1.5010 - acc: 0.6033 - val_loss: 1.4134 - val_acc: 0.6874\n",
      "Epoch 14/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 1.4663 - acc: 0.5993 - val_loss: 1.3648 - val_acc: 0.6874\n",
      "Epoch 15/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 1.4337 - acc: 0.6025 - val_loss: 1.3315 - val_acc: 0.6874\n",
      "Epoch 16/100\n",
      "9125/9125 [==============================] - 41s 5ms/step - loss: 1.3989 - acc: 0.6045 - val_loss: 1.2909 - val_acc: 0.6874\n",
      "Epoch 17/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 1.3595 - acc: 0.6088 - val_loss: 1.2624 - val_acc: 0.6874\n",
      "Epoch 18/100\n",
      "9125/9125 [==============================] - 43s 5ms/step - loss: 1.3398 - acc: 0.6041 - val_loss: 1.2196 - val_acc: 0.6874\n",
      "Epoch 19/100\n",
      "9125/9125 [==============================] - 48s 5ms/step - loss: 1.3057 - acc: 0.6069 - val_loss: 1.1942 - val_acc: 0.6874\n",
      "Epoch 20/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 1.2767 - acc: 0.6089 - val_loss: 1.1654 - val_acc: 0.6874\n",
      "Epoch 21/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 1.2455 - acc: 0.6111 - val_loss: 1.1375 - val_acc: 0.6874\n",
      "Epoch 22/100\n",
      "9125/9125 [==============================] - 37s 4ms/step - loss: 1.2376 - acc: 0.6081 - val_loss: 1.1123 - val_acc: 0.6874\n",
      "Epoch 23/100\n",
      "9125/9125 [==============================] - 34s 4ms/step - loss: 1.2014 - acc: 0.6164 - val_loss: 1.0781 - val_acc: 0.6893\n",
      "Epoch 24/100\n",
      "9125/9125 [==============================] - 36s 4ms/step - loss: 1.1850 - acc: 0.6114 - val_loss: 1.0578 - val_acc: 0.6893\n",
      "Epoch 25/100\n",
      "9125/9125 [==============================] - 41s 5ms/step - loss: 1.1627 - acc: 0.6142 - val_loss: 1.0365 - val_acc: 0.6893\n",
      "Epoch 26/100\n",
      "9125/9125 [==============================] - 37s 4ms/step - loss: 1.1347 - acc: 0.6185 - val_loss: 1.0198 - val_acc: 0.6893\n",
      "Epoch 27/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 1.1353 - acc: 0.6124 - val_loss: 0.9939 - val_acc: 0.6893\n",
      "Epoch 28/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 1.0993 - acc: 0.6175 - val_loss: 0.9818 - val_acc: 0.6893\n",
      "Epoch 29/100\n",
      "9125/9125 [==============================] - 37s 4ms/step - loss: 1.0813 - acc: 0.6192 - val_loss: 0.9622 - val_acc: 0.6884\n",
      "Epoch 30/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 1.0826 - acc: 0.6146 - val_loss: 0.9400 - val_acc: 0.6874\n",
      "Epoch 31/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 1.0655 - acc: 0.6197 - val_loss: 0.9224 - val_acc: 0.6884\n",
      "Epoch 32/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 1.0382 - acc: 0.6230 - val_loss: 0.9092 - val_acc: 0.6874\n",
      "Epoch 33/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 1.0331 - acc: 0.6245 - val_loss: 0.8943 - val_acc: 0.6864\n",
      "Epoch 34/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 1.0293 - acc: 0.6247 - val_loss: 0.8799 - val_acc: 0.6874\n",
      "Epoch 35/100\n",
      "9125/9125 [==============================] - 37s 4ms/step - loss: 1.0094 - acc: 0.6256 - val_loss: 0.8716 - val_acc: 0.6864\n",
      "Epoch 36/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 0.9999 - acc: 0.6289 - val_loss: 0.8595 - val_acc: 0.6874\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9125/9125 [==============================] - 38s 4ms/step - loss: 0.9890 - acc: 0.6283 - val_loss: 0.8459 - val_acc: 0.6874\n",
      "Epoch 38/100\n",
      "9125/9125 [==============================] - 36s 4ms/step - loss: 0.9684 - acc: 0.6302 - val_loss: 0.8334 - val_acc: 0.6874\n",
      "Epoch 39/100\n",
      "9125/9125 [==============================] - 36s 4ms/step - loss: 0.9759 - acc: 0.6238 - val_loss: 0.8238 - val_acc: 0.6874\n",
      "Epoch 40/100\n",
      "9125/9125 [==============================] - 36s 4ms/step - loss: 0.9650 - acc: 0.6307 - val_loss: 0.8167 - val_acc: 0.6874\n",
      "Epoch 41/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 0.9600 - acc: 0.6342 - val_loss: 0.8066 - val_acc: 0.6864\n",
      "Epoch 42/100\n",
      "9125/9125 [==============================] - 37s 4ms/step - loss: 0.9463 - acc: 0.6327 - val_loss: 0.8013 - val_acc: 0.6874\n",
      "Epoch 43/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 0.9326 - acc: 0.6384 - val_loss: 0.7920 - val_acc: 0.6874\n",
      "Epoch 44/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.9173 - acc: 0.6411 - val_loss: 0.7891 - val_acc: 0.6874\n",
      "Epoch 45/100\n",
      "9125/9125 [==============================] - 37s 4ms/step - loss: 0.9107 - acc: 0.6401 - val_loss: 0.7797 - val_acc: 0.6874\n",
      "Epoch 46/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.9164 - acc: 0.6357 - val_loss: 0.7736 - val_acc: 0.6874\n",
      "Epoch 47/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.9067 - acc: 0.6355 - val_loss: 0.7679 - val_acc: 0.6874\n",
      "Epoch 48/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 0.9008 - acc: 0.6368 - val_loss: 0.7663 - val_acc: 0.6874\n",
      "Epoch 49/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.8857 - acc: 0.6413 - val_loss: 0.7612 - val_acc: 0.6884\n",
      "Epoch 50/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.8907 - acc: 0.6395 - val_loss: 0.7564 - val_acc: 0.6884\n",
      "Epoch 51/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.8771 - acc: 0.6430 - val_loss: 0.7533 - val_acc: 0.6884\n",
      "Epoch 52/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.8743 - acc: 0.6405 - val_loss: 0.7491 - val_acc: 0.6874\n",
      "Epoch 53/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.8641 - acc: 0.6445 - val_loss: 0.7453 - val_acc: 0.6982\n",
      "Epoch 54/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.8650 - acc: 0.6391 - val_loss: 0.7390 - val_acc: 0.6893\n",
      "Epoch 55/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.8616 - acc: 0.6401 - val_loss: 0.7387 - val_acc: 0.6893\n",
      "Epoch 56/100\n",
      "9125/9125 [==============================] - 43s 5ms/step - loss: 0.8445 - acc: 0.6483 - val_loss: 0.7392 - val_acc: 0.6992\n",
      "Epoch 57/100\n",
      "9125/9125 [==============================] - 41s 4ms/step - loss: 0.8446 - acc: 0.6460 - val_loss: 0.7343 - val_acc: 0.6903\n",
      "Epoch 58/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.8399 - acc: 0.6434 - val_loss: 0.7357 - val_acc: 0.7012\n",
      "Epoch 59/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.8429 - acc: 0.6404 - val_loss: 0.7314 - val_acc: 0.6933\n",
      "Epoch 60/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.8178 - acc: 0.6513 - val_loss: 0.7273 - val_acc: 0.7022\n",
      "Epoch 61/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.8060 - acc: 0.6505 - val_loss: 0.7271 - val_acc: 0.7022\n",
      "Epoch 62/100\n",
      "9125/9125 [==============================] - 41s 5ms/step - loss: 0.8057 - acc: 0.6505 - val_loss: 0.7220 - val_acc: 0.7041\n",
      "Epoch 63/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.7953 - acc: 0.6490 - val_loss: 0.7205 - val_acc: 0.7071\n",
      "Epoch 64/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.7976 - acc: 0.6502 - val_loss: 0.7151 - val_acc: 0.7071\n",
      "Epoch 65/100\n",
      "9125/9125 [==============================] - 43s 5ms/step - loss: 0.7940 - acc: 0.6495 - val_loss: 0.7135 - val_acc: 0.7051\n",
      "Epoch 66/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.7783 - acc: 0.6568 - val_loss: 0.7167 - val_acc: 0.7061\n",
      "Epoch 67/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.7668 - acc: 0.6585 - val_loss: 0.7082 - val_acc: 0.7061\n",
      "Epoch 68/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.7678 - acc: 0.6556 - val_loss: 0.7062 - val_acc: 0.7051\n",
      "Epoch 69/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.7573 - acc: 0.6635 - val_loss: 0.7053 - val_acc: 0.7061\n",
      "Epoch 70/100\n",
      "9125/9125 [==============================] - 50s 5ms/step - loss: 0.7473 - acc: 0.6654 - val_loss: 0.7056 - val_acc: 0.7091\n",
      "Epoch 71/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.7513 - acc: 0.6613 - val_loss: 0.7038 - val_acc: 0.7101\n",
      "Epoch 72/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.7491 - acc: 0.6671 - val_loss: 0.7026 - val_acc: 0.7101\n",
      "Epoch 73/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.7416 - acc: 0.6683 - val_loss: 0.6993 - val_acc: 0.7150\n",
      "Epoch 74/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.7433 - acc: 0.6666 - val_loss: 0.6979 - val_acc: 0.7160\n",
      "Epoch 75/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.7376 - acc: 0.6712 - val_loss: 0.6970 - val_acc: 0.7179\n",
      "Epoch 76/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.7242 - acc: 0.6770 - val_loss: 0.6931 - val_acc: 0.7160\n",
      "Epoch 77/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.7177 - acc: 0.6785 - val_loss: 0.6919 - val_acc: 0.7170\n",
      "Epoch 78/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.7281 - acc: 0.6729 - val_loss: 0.6960 - val_acc: 0.7179\n",
      "Epoch 79/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.7213 - acc: 0.6797 - val_loss: 0.6942 - val_acc: 0.7189\n",
      "Epoch 80/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.7153 - acc: 0.6799 - val_loss: 0.6893 - val_acc: 0.7239\n",
      "Epoch 81/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.7154 - acc: 0.6805 - val_loss: 0.6913 - val_acc: 0.7249\n",
      "Epoch 82/100\n",
      "9125/9125 [==============================] - 44s 5ms/step - loss: 0.7139 - acc: 0.6845 - val_loss: 0.6891 - val_acc: 0.7219\n",
      "Epoch 83/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.7053 - acc: 0.6858 - val_loss: 0.6898 - val_acc: 0.7239\n",
      "Epoch 84/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.6983 - acc: 0.6941 - val_loss: 0.6847 - val_acc: 0.7239\n",
      "Epoch 85/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.6921 - acc: 0.6883 - val_loss: 0.6838 - val_acc: 0.7239\n",
      "Epoch 86/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.6943 - acc: 0.6940 - val_loss: 0.6899 - val_acc: 0.7239\n",
      "Epoch 87/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.6841 - acc: 0.6971 - val_loss: 0.6893 - val_acc: 0.7229\n",
      "Epoch 88/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.6777 - acc: 0.6973 - val_loss: 0.6856 - val_acc: 0.7249\n",
      "Epoch 89/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.6806 - acc: 0.6970 - val_loss: 0.6843 - val_acc: 0.7278\n",
      "Epoch 90/100\n",
      "9125/9125 [==============================] - 39s 4ms/step - loss: 0.6757 - acc: 0.7010 - val_loss: 0.6831 - val_acc: 0.7268\n",
      "Epoch 91/100\n",
      "9125/9125 [==============================] - 42s 5ms/step - loss: 0.6751 - acc: 0.6998 - val_loss: 0.6840 - val_acc: 0.7268\n",
      "Epoch 92/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.6669 - acc: 0.7022 - val_loss: 0.6785 - val_acc: 0.7278\n",
      "Epoch 93/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.6703 - acc: 0.6988 - val_loss: 0.6817 - val_acc: 0.7318\n",
      "Epoch 94/100\n",
      "9125/9125 [==============================] - 43s 5ms/step - loss: 0.6584 - acc: 0.7030 - val_loss: 0.6785 - val_acc: 0.7327\n",
      "Epoch 95/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.6591 - acc: 0.7048 - val_loss: 0.6796 - val_acc: 0.7308\n",
      "Epoch 96/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.6533 - acc: 0.7038 - val_loss: 0.6775 - val_acc: 0.7318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      "9125/9125 [==============================] - 48s 5ms/step - loss: 0.6554 - acc: 0.7082 - val_loss: 0.6791 - val_acc: 0.7347\n",
      "Epoch 98/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 0.6547 - acc: 0.7054 - val_loss: 0.6738 - val_acc: 0.7357\n",
      "Epoch 99/100\n",
      "9125/9125 [==============================] - 38s 4ms/step - loss: 0.6434 - acc: 0.7110 - val_loss: 0.6735 - val_acc: 0.7387\n",
      "Epoch 100/100\n",
      "9125/9125 [==============================] - 40s 4ms/step - loss: 0.6421 - acc: 0.7144 - val_loss: 0.6731 - val_acc: 0.7367\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in training_data:\n",
    "    print(i['document_matrix'].shape)\n",
    "    model = simple_ffn(i['document_matrix'], i['labels'])\n",
    "    model.fit(i['document_matrix'], i['labels'], epochs=100, validation_split=0.1)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(os.path.join(MODELS_PATH,'ffn_sample_model_sentences_bi.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on unseen data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for i in training_data:\n",
    "    X_test, y_test = prepare_test_data(corpora_test, labels_test, i['pipeline_instance'])\n",
    "    test_data.append({'X_test': X_test, 'y_test': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>\"</th>\n",
       "      <th>#</th>\n",
       "      <th>$</th>\n",
       "      <th>%</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>'</th>\n",
       "      <th>(</th>\n",
       "      <th>)</th>\n",
       "      <th>...</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>w</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>{</th>\n",
       "      <th>|</th>\n",
       "      <th>}</th>\n",
       "      <th>~</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.351427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080407</td>\n",
       "      <td>0.080444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.168340</td>\n",
       "      <td>0.118161</td>\n",
       "      <td>0.178917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.531607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038198</td>\n",
       "      <td>0.268115</td>\n",
       "      <td>0.121792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038955</td>\n",
       "      <td>0.054692</td>\n",
       "      <td>0.066025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.231594</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188653</td>\n",
       "      <td>0.151879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.380765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075985</td>\n",
       "      <td>0.065340</td>\n",
       "      <td>0.065370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336291</td>\n",
       "      <td>0.018886</td>\n",
       "      <td>0.054718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087234</td>\n",
       "      <td>0.070230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               !    \"    #    $    %    &         '         (         ) ...   \\\n",
       "0  0.351427  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.080407  0.080444 ...    \n",
       "1  0.531607  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000 ...    \n",
       "2  0.470930  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000 ...    \n",
       "3  0.231594  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000  0.000000 ...    \n",
       "4  0.380765  0.0  0.0  0.0  0.0  0.0  0.0  0.075985  0.065340  0.065370 ...    \n",
       "\n",
       "          u         v         w         x         y         z    {    |    }  \\\n",
       "0  0.198643  0.000000  0.168340  0.118161  0.178917  0.000000  0.0  0.0  0.0   \n",
       "1  0.037561  0.000000  0.038198  0.268115  0.121792  0.000000  0.0  0.0  0.0   \n",
       "2  0.038955  0.054692  0.066025  0.000000  0.014035  0.000000  0.0  0.0  0.0   \n",
       "3  0.087272  0.000000  0.266251  0.000000  0.188653  0.151879  0.0  0.0  0.0   \n",
       "4  0.336291  0.018886  0.054718  0.000000  0.087234  0.070230  0.0  0.0  0.0   \n",
       "\n",
       "     ~  \n",
       "0  0.0  \n",
       "1  0.0  \n",
       "2  0.0  \n",
       "3  0.0  \n",
       "4  0.0  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]['X_test'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>af</th>\n",
       "      <th>en</th>\n",
       "      <th>nr</th>\n",
       "      <th>nso</th>\n",
       "      <th>ss</th>\n",
       "      <th>st</th>\n",
       "      <th>tn</th>\n",
       "      <th>ts</th>\n",
       "      <th>ve</th>\n",
       "      <th>xh</th>\n",
       "      <th>zu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34750</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18986</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13655</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15126</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21978</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       af  en  nr  nso  ss  st  tn  ts  ve  xh  zu\n",
       "34750   0   0   0    0   0   0   0   0   0   1   0\n",
       "18986   0   0   0    1   0   0   0   0   0   0   0\n",
       "13655   0   1   0    0   0   0   0   0   0   0   0\n",
       "15126   0   0   1    0   0   0   0   0   0   0   0\n",
       "21978   0   0   0    0   1   0   0   0   0   0   0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]['y_test'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30417/30417 [==============================] - 1s 28us/step\n",
      "Model test accuracy 77.2\n",
      "30417/30417 [==============================] - 2s 51us/step\n",
      "Model test accuracy 75.77000000000001\n",
      "30417/30417 [==============================] - 15s 504us/step\n",
      "Model test accuracy 71.95\n"
     ]
    }
   ],
   "source": [
    "for idx, i in enumerate(test_data):\n",
    "    score, accuracy = models[idx].evaluate(i['X_test'], i['y_test'])\n",
    "    print('Model test accuracy', accuracy.round(4)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sa_lang",
   "language": "python",
   "name": "sa_lang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
